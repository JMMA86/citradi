{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lxml[html_clean]\n",
    "%pip install newspaper3k\n",
    "%pip install praw\n",
    "%pip install PyGithub\n",
    "%pip install pandas\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from github import Github\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "import newspaper\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from newspaper import Article, Config\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import praw\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load keywords (change to the csv you want)\n",
    "keywords_csv_path = os.getenv(\"KEYWORDS_CSV\")\n",
    "df_keywords = pd.read_csv(keywords_csv_path, header=None)\n",
    "globalKeywords = [str(item) for sublist in df_keywords.values.tolist() for item in sublist if pd.notnull(item)]\n",
    "print(\"Global AI Keywords:\", globalKeywords)\n",
    "\n",
    "# Themes loader (change to the csv you want)\n",
    "themes_csv_path = os.getenv(\"THEMES_CSV\")\n",
    "\n",
    "# GitHub personal access token\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Reddit API credentials\n",
    "reddit_credentials = {\n",
    "    \"client_id\": os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    \"client_secret\": os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    \"user_agent\": os.getenv(\"REDDIT_USER_AGENT\"),\n",
    "}\n",
    "\n",
    "# Subreddits to scrape\n",
    "subreddits = os.getenv(\"REDDIT_SUBREDDITS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThemeProcessor:\n",
    "    @staticmethod\n",
    "    def process_keywords_from_csv(csv_path):\n",
    "        \"\"\"Process keywords from CSV file\"\"\"\n",
    "        df = pd.read_csv(\n",
    "            csv_path, sep=\";\", header=None\n",
    "        )  # Asumimos que no hay headers, si los hay, ajustar\n",
    "        themes_dict = defaultdict(list)\n",
    "\n",
    "        # Por cada columna en el DataFrame\n",
    "        for col in df.columns:\n",
    "            current_theme = None\n",
    "            for val in df[col].dropna():\n",
    "                line = str(val).strip()\n",
    "\n",
    "                # Si la línea inicia con '===' y finaliza con '===', es un nuevo tema\n",
    "                if line.startswith(\"===\") and line.endswith(\"===\"):\n",
    "                    # Extraemos el nombre del tema quitando los '==='\n",
    "                    # Por ejemplo: === Inclusive growth, sustainable development and well-being ===\n",
    "                    # Queremos quedarnos solo con el texto interno\n",
    "                    theme_name = line.strip(\"=\").strip()\n",
    "                    current_theme = theme_name\n",
    "                    if current_theme not in themes_dict:\n",
    "                        themes_dict[current_theme] = []\n",
    "\n",
    "                # Si la línea empieza con '-', es un subtema asociado al tema actual\n",
    "                elif line.startswith(\"-\") and current_theme:\n",
    "                    subtopic = line.lstrip(\"-\").strip()\n",
    "                    if subtopic:\n",
    "                        themes_dict[current_theme].append(subtopic)\n",
    "\n",
    "        # Remover duplicados en las listas (opcional)\n",
    "        for theme in themes_dict:\n",
    "            themes_dict[theme] = list(set(themes_dict[theme]))\n",
    "\n",
    "        return dict(themes_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AINewsAnalyzer:\n",
    "    def __init__(self, themes_csv_path, news_sources=None):\n",
    "        \"\"\"\n",
    "        Initialize the AI News Analyzer with themes from CSV\n",
    "\n",
    "        Args:\n",
    "            themes_csv_path (str): Path to CSV file containing themes and keywords\n",
    "            news_sources (list): Optional list of news sources to analyze\n",
    "        \"\"\"\n",
    "        self.themes = ThemeProcessor.process_keywords_from_csv(themes_csv_path)\n",
    "        self.news_sources = news_sources or [\n",
    "            \"https://efe.com/en/\",\n",
    "            \"http://www.wired.com\",\n",
    "            \"http://www.bbc.com\",\n",
    "            \"http://www.cnn.com\",\n",
    "            \"http://www.reuters.com\",\n",
    "            \"http://www.theguardian.com\",\n",
    "            \"http://www.nytimes.com\",\n",
    "            \"https://www.afp.com\",\n",
    "            \"https://www.wired.com\",\n",
    "            \"https://www.theguardian.com/technology\",\n",
    "        ]\n",
    "\n",
    "        self.ai_related_terms = globalKeywords\n",
    "\n",
    "        self.articles_data = []\n",
    "        self.debug_stats = {\n",
    "            \"total_urls_found\": 0,\n",
    "            \"download_failures\": 0,\n",
    "            \"parsing_failures\": 0,\n",
    "            \"ai_related_found\": 0,\n",
    "            \"theme_matched\": 0,\n",
    "        }\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        log_dir = Path(\"logs\")\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / \"ai_news_analyzer.log\"),\n",
    "                logging.StreamHandler(),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def is_ai_related(self, text, title):\n",
    "        \"\"\"Check if article is AI-related and matches themes\"\"\"\n",
    "        combined_text = (text + \" \" + title).lower()\n",
    "\n",
    "        # First check if it's AI-related\n",
    "        found_keywords = [\n",
    "            term\n",
    "            for term in self.ai_related_terms\n",
    "            if re.search(r\"\\b\" + re.escape(term.lower()) + r\"\\b\", combined_text)\n",
    "        ]\n",
    "\n",
    "        if not found_keywords:\n",
    "            return False\n",
    "\n",
    "        print(\"AI word:\", found_keywords)\n",
    "\n",
    "        # Then check if it matches any of our theme keywords\n",
    "        for theme, keywords in self.themes.items():\n",
    "            if any(\n",
    "                re.search(r\"\\b\" + re.escape(keyword.lower()) + r\"\\b\", combined_text)\n",
    "                for keyword in keywords\n",
    "            ):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def download_and_parse_article(self, article_url):\n",
    "        \"\"\"Download and parse a single article with better error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nAttempting to process: {article_url}\")  # Debug print\n",
    "\n",
    "            article = Article(article_url)\n",
    "            try:\n",
    "                article.download()\n",
    "                time.sleep(1)  # Increased delay to be more polite\n",
    "            except Exception as e:\n",
    "                self.debug_stats[\"download_failures\"] += 1\n",
    "                print(f\"Download failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                article.parse()\n",
    "                article.nlp()\n",
    "            except Exception as e:\n",
    "                self.debug_stats[\"parsing_failures\"] += 1\n",
    "                print(f\"Parsing failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "            # Check if we got actual content\n",
    "            if not article.text or len(article.text) < 100:\n",
    "                print(\"Article too short or empty\")\n",
    "                return None\n",
    "\n",
    "            if self.is_ai_related(article.text, article.title):\n",
    "                self.debug_stats[\"ai_related_found\"] += 1\n",
    "                print(\"AI-related article found!\")\n",
    "\n",
    "                # Match themes and keywords\n",
    "                matched_themes = {}\n",
    "                text = (article.text + \" \" + article.title).lower()\n",
    "\n",
    "                for theme, keywords in self.themes.items():\n",
    "                    matched_keywords = []\n",
    "                    for keyword in keywords:\n",
    "                        if keyword.lower() in text:\n",
    "                            matched_keywords.append(keyword)\n",
    "                    if matched_keywords:\n",
    "                        matched_themes[theme] = matched_keywords\n",
    "\n",
    "                if matched_themes:\n",
    "                    self.debug_stats[\"theme_matched\"] += 1\n",
    "                    return {\n",
    "                        \"url\": article_url,\n",
    "                        \"title\": article.title,\n",
    "                        \"text\": article.text,\n",
    "                        \"summary\": article.summary,\n",
    "                        \"keywords\": article.keywords,\n",
    "                        \"publish_date\": article.publish_date.strftime(\"%Y-%m-%d\")\n",
    "                        if article.publish_date\n",
    "                        else None,\n",
    "                        \"authors\": article.authors,\n",
    "                        \"matched_themes\": matched_themes,\n",
    "                        \"source\": re.findall(\n",
    "                            r\"https?://(?:www\\.)?([^/]+)\", article_url\n",
    "                        )[0],\n",
    "                    }\n",
    "            else:\n",
    "                print(\"Not AI-related\")\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {article_url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sources(self, max_articles_per_source=50, start_year=2014):\n",
    "        \"\"\"Analyze news sources with better debugging\"\"\"\n",
    "        for source_url in tqdm(self.news_sources, desc=\"Processing news sources\"):\n",
    "            try:\n",
    "                print(f\"\\nProcessing source: {source_url}\")\n",
    "\n",
    "                config = Config()\n",
    "                config.request_timeout = 30  # Increased timeout\n",
    "                config.memoize_articles = False\n",
    "                config.fetch_images = False\n",
    "\n",
    "                # Build source object\n",
    "                source = newspaper.build(\n",
    "                    source_url, config=config, language=\"en\", number_threads=1\n",
    "                )\n",
    "\n",
    "                print(f\"Found {len(source.articles)} articles at {source_url}\")\n",
    "                self.debug_stats[\"total_urls_found\"] += len(source.articles)\n",
    "\n",
    "                # Get article URLs\n",
    "                ai_related_urls = []\n",
    "                normalized_urls_set = set()\n",
    "                for article in tqdm(\n",
    "                    source.articles[: max_articles_per_source * 2],\n",
    "                    desc=f\"Scanning articles from {source_url}\",\n",
    "                ):\n",
    "                    if not article.url:\n",
    "                        continue\n",
    "\n",
    "                    # Normalize url\n",
    "                    normalized_url = re.sub(r\"^https?://\", \"\", article.url)\n",
    "\n",
    "                    # Avoid duplicates\n",
    "                    if normalized_url in normalized_urls_set:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "\n",
    "                        if not article.text or len(article.text) < 100:\n",
    "                            continue\n",
    "\n",
    "                        if article.publish_date:\n",
    "                            article_year = article.publish_date.year\n",
    "                            if start_year <= article_year <= datetime.now().year:\n",
    "                                if self.is_ai_related(article.text, article.title):\n",
    "                                    ai_related_urls.append(article.url)\n",
    "                                    normalized_urls_set.add(normalized_url)\n",
    "                                    print(f\"Found AI article: {article.url}\")\n",
    "\n",
    "                        if len(ai_related_urls) >= max_articles_per_source:\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing article: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"Found {len(ai_related_urls)} AI-related articles\")\n",
    "\n",
    "                # Process the found articles\n",
    "                with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "                    results = list(\n",
    "                        executor.map(self.download_and_parse_article, ai_related_urls)\n",
    "                    )\n",
    "\n",
    "                valid_results = [r for r in results if r is not None]\n",
    "                self.articles_data.extend(valid_results)\n",
    "\n",
    "                print(f\"Successfully processed {len(valid_results)} articles\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing source {source_url}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def analyze_content(self):\n",
    "        \"\"\"Analyze collected articles for themes\"\"\"\n",
    "        analysis_results = defaultdict(lambda: defaultdict(int))\n",
    "        articles_by_theme = defaultdict(list)\n",
    "\n",
    "        for article in tqdm(self.articles_data, desc=\"Analyzing articles\"):\n",
    "            for theme, keywords in article[\"matched_themes\"].items():\n",
    "                analysis_results[theme][\"articles_count\"] += 1\n",
    "                analysis_results[theme][\"keyword_occurrences\"] += len(keywords)\n",
    "\n",
    "                articles_by_theme[theme].append(\n",
    "                    {\n",
    "                        \"url\": article[\"url\"],\n",
    "                        \"title\": article[\"title\"],\n",
    "                        \"publish_date\": article[\"publish_date\"],\n",
    "                        \"keywords_found\": keywords,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return analysis_results, articles_by_theme\n",
    "\n",
    "    def save_results(\n",
    "        self, analysis_results, articles_by_theme, output_prefix=\"ai_news_analysis\"\n",
    "    ):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        from collections import defaultdict\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        output_dir = Path(\"results\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Deduplicate articles in each theme (by title)\n",
    "        deduped_articles_by_theme = defaultdict(list)\n",
    "        for theme, articles in articles_by_theme.items():\n",
    "            seen_titles = set()\n",
    "            for article in articles:\n",
    "                if article[\"title\"] not in seen_titles:\n",
    "                    deduped_articles_by_theme[theme].append(article)\n",
    "                    seen_titles.add(article[\"title\"])\n",
    "\n",
    "        # Create complete report with deduped articles\n",
    "        report = {\n",
    "            \"summary\": {\n",
    "                \"total_articles\": len(self.articles_data),\n",
    "                \"analysis_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"themes_analyzed\": list(self.themes.keys()),\n",
    "            },\n",
    "            \"theme_analysis\": {\n",
    "                theme: {\n",
    "                    # Use deduplicated article count\n",
    "                    \"articles_count\": len(deduped_articles_by_theme[theme]),\n",
    "                    \"keyword_occurrences\": data[\"keyword_occurrences\"],\n",
    "                    \"articles\": deduped_articles_by_theme[theme],\n",
    "                }\n",
    "                for theme, data in analysis_results.items()\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Save JSON report\n",
    "        with open(\n",
    "            output_dir / f\"{output_prefix}_report.json\", \"w\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Create theme analysis DataFrame (deduped counts)\n",
    "        theme_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"theme\": theme,\n",
    "                    \"articles_count\": len(deduped_articles_by_theme[theme]),\n",
    "                    \"keyword_occurrences\": data[\"keyword_occurrences\"],\n",
    "                    \"percentage_of_total\": (\n",
    "                        len(deduped_articles_by_theme[theme])\n",
    "                        / len(self.articles_data)\n",
    "                        * 100\n",
    "                    )\n",
    "                    if self.articles_data\n",
    "                    else 0,\n",
    "                }\n",
    "                for theme, data in analysis_results.items()\n",
    "            ]\n",
    "        )\n",
    "        theme_df.to_csv(output_dir / f\"{output_prefix}_theme_analysis.csv\", index=False)\n",
    "\n",
    "        # Create articles DataFrame (deduped)\n",
    "        articles_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"theme\": theme,\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"url\": article[\"url\"],\n",
    "                    \"publish_date\": article[\"publish_date\"],\n",
    "                    \"keywords\": \", \".join(article[\"keywords_found\"]),\n",
    "                }\n",
    "                for theme, articles in deduped_articles_by_theme.items()\n",
    "                for article in articles\n",
    "            ]\n",
    "        )\n",
    "        articles_df.to_csv(output_dir / f\"{output_prefix}_articles.csv\", index=False)\n",
    "\n",
    "        logging.info(\n",
    "            f\"Results saved in 'results' directory with prefix '{output_prefix}'\"\n",
    "        )\n",
    "        return report\n",
    "\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize analyzer with your CSV file\n",
    "    analyzer = AINewsAnalyzer(themes_csv_path)\n",
    "\n",
    "    # Analyze sources\n",
    "    analyzer.analyze_sources(max_articles_per_source=100, start_year=2022)\n",
    "\n",
    "    # Analyze content\n",
    "    analysis_results, articles_by_theme = analyzer.analyze_content()\n",
    "\n",
    "    # Save and get report\n",
    "    report = analyzer.save_results(analysis_results, articles_by_theme)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"Total articles analyzed: {report['summary']['total_articles']}\")\n",
    "    print(\"\\nResults by theme:\")\n",
    "    for theme, data in report[\"theme_analysis\"].items():\n",
    "        print(f\"\\n{theme}:\")\n",
    "        print(f\"  Articles: {data['articles_count']}\")\n",
    "        print(f\"  Keyword occurrences: {data['keyword_occurrences']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForumAnalyzer:\n",
    "    def __init__(self, themes_csv_path, reddit_credentials=None, github_token=None):\n",
    "        \"\"\"\n",
    "        Initialize the Forum Analyzer\n",
    "\n",
    "        Args:\n",
    "            themes_csv_path (str): Path to CSV file containing themes and keywords\n",
    "            reddit_credentials (dict): Dictionary with Reddit API credentials\n",
    "            github_token (str): GitHub personal access token\n",
    "        \"\"\"\n",
    "        self.themes = ThemeProcessor.process_keywords_from_csv(themes_csv_path)\n",
    "        self.articles_data = []\n",
    "\n",
    "        # Initialize Reddit client if credentials provided\n",
    "        self.reddit = None\n",
    "        if reddit_credentials:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=reddit_credentials[\"client_id\"],\n",
    "                client_secret=reddit_credentials[\"client_secret\"],\n",
    "                user_agent=reddit_credentials[\"user_agent\"],\n",
    "            )\n",
    "\n",
    "        # Initialize GitHub client if token provided\n",
    "        self.github = None\n",
    "        if github_token:\n",
    "            self.github = Github(github_token)\n",
    "\n",
    "        # Initialize logging\n",
    "        self.setup_logging()\n",
    "\n",
    "        # AI-related terms (inherited from AINewsAnalyzer)\n",
    "        self.ai_related_terms = globalKeywords\n",
    "\n",
    "        # Debug stats\n",
    "        self.debug_stats = defaultdict(int)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        log_dir = Path(\"logs\")\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / \"forum_analyzer.log\"),\n",
    "                logging.StreamHandler(),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def is_ai_related(self, text, title=\"\"):\n",
    "        \"\"\"Check if content is AI-related and matches themes\"\"\"\n",
    "        combined_text = (text + \" \" + title).lower()\n",
    "\n",
    "        # First check if it's AI-related\n",
    "        if not any(term.lower() in combined_text for term in self.ai_related_terms):\n",
    "            return False\n",
    "\n",
    "        # Then check if it matches any of our theme keywords\n",
    "        for theme, keywords in self.themes.items():\n",
    "            if any(keyword.lower() in combined_text for keyword in keywords):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def analyze_reddit(\n",
    "        self,\n",
    "        subreddits=subreddits.split(\",\") if subreddits else [\"all\"],\n",
    "        time_filter=\"year\",\n",
    "        limit=1000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Analyze Reddit posts from specified subreddits\n",
    "\n",
    "        Args:\n",
    "            subreddits (list): List of subreddit names to analyze\n",
    "            time_filter (str): One of 'day', 'week', 'month', 'year', 'all'\n",
    "            limit (int): Maximum number of posts to analyze per subreddit\n",
    "        \"\"\"\n",
    "        if not self.reddit:\n",
    "            logging.error(\"Reddit client not initialized. Please provide credentials.\")\n",
    "            return\n",
    "\n",
    "        for subreddit_name in tqdm(subreddits, desc=\"Processing subreddits\"):\n",
    "            try:\n",
    "                subreddit = self.reddit.subreddit(subreddit_name)\n",
    "\n",
    "                # Get top posts\n",
    "                for post in tqdm(\n",
    "                    subreddit.top(time_filter=time_filter, limit=limit),\n",
    "                    desc=f\"Analyzing posts from r/{subreddit_name}\",\n",
    "                ):\n",
    "\n",
    "                    # Combine post title, content and top comments\n",
    "                    post_text = f\"{post.title} {post.selftext}\"\n",
    "\n",
    "                    # Add top comments\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    comments_text = \" \".join(\n",
    "                        [comment.body for comment in post.comments.list()[:10]]\n",
    "                    )\n",
    "\n",
    "                    combined_text = post_text + \" \" + comments_text\n",
    "\n",
    "                    if self.is_ai_related(combined_text, post.title):\n",
    "                        self.debug_stats[\"reddit_ai_related\"] += 1\n",
    "\n",
    "                        # Match themes and keywords\n",
    "                        matched_themes = {}\n",
    "                        text = combined_text.lower()\n",
    "\n",
    "                        for theme, keywords in self.themes.items():\n",
    "                            matched_keywords = [\n",
    "                                k for k in keywords if k.lower() in text\n",
    "                            ]\n",
    "                            if matched_keywords:\n",
    "                                matched_themes[theme] = matched_keywords\n",
    "\n",
    "                        if matched_themes:\n",
    "                            self.articles_data.append(\n",
    "                                {\n",
    "                                    \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "                                    \"title\": post.title,\n",
    "                                    \"text\": combined_text,\n",
    "                                    \"summary\": post.selftext[:500]\n",
    "                                    if post.selftext\n",
    "                                    else \"\",\n",
    "                                    \"publish_date\": datetime.fromtimestamp(\n",
    "                                        post.created_utc\n",
    "                                    ).strftime(\"%Y-%m-%d\"),\n",
    "                                    \"author\": str(post.author),\n",
    "                                    \"matched_themes\": matched_themes,\n",
    "                                    \"source\": f\"reddit/r/{subreddit_name}\",\n",
    "                                    \"score\": post.score,\n",
    "                                    \"num_comments\": post.num_comments,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing subreddit {subreddit_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def analyze_github(\n",
    "        self,\n",
    "        query=\"artificial intelligence\",\n",
    "        sort=\"stars\",\n",
    "        max_repos=100,\n",
    "        min_stars=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Analyze GitHub repositories (description and readme only, omite discussions for compatibility)\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query for repositories\n",
    "            sort (str): How to sort results ('stars', 'forks', 'updated')\n",
    "            max_repos (int): Maximum number of repositories to analyze\n",
    "            min_stars (int): Minimum number of stars for a repository\n",
    "        \"\"\"\n",
    "        if not self.github:\n",
    "            logging.error(\"GitHub client not initialized. Please provide token.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Search repositories\n",
    "            repositories = self.github.search_repositories(\n",
    "                query=f\"{query} stars:>={min_stars}\", sort=sort, order=\"desc\"\n",
    "            )\n",
    "\n",
    "            for repo in tqdm(\n",
    "                repositories[:max_repos], desc=\"Analyzing GitHub repositories\"\n",
    "            ):\n",
    "                try:\n",
    "                    # Combine repository description and readme (omit discussions for compatibility)\n",
    "                    repo_text = f\"{repo.description or ''}\"\n",
    "\n",
    "                    try:\n",
    "                        readme = repo.get_readme().decoded_content.decode()\n",
    "                        repo_text += \" \" + readme\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Get discussions if available (doesn't work)\n",
    "                    # if repo.has_discussions:\n",
    "                    #     discussions = repo.get_discussions()\n",
    "                    #     for discussion in discussions[:10]:  # Get first 10 discussions\n",
    "                    #         repo_text += f\" {discussion.title} {discussion.body}\"\n",
    "\n",
    "                    if self.is_ai_related(repo_text, repo.name):\n",
    "                        self.debug_stats[\"github_ai_related\"] += 1\n",
    "\n",
    "                        # Match themes and keywords\n",
    "                        matched_themes = {}\n",
    "                        text = repo_text.lower()\n",
    "\n",
    "                        for theme, keywords in self.themes.items():\n",
    "                            matched_keywords = [\n",
    "                                k for k in keywords if k.lower() in text\n",
    "                            ]\n",
    "                            if matched_keywords:\n",
    "                                matched_themes[theme] = matched_keywords\n",
    "\n",
    "                        if matched_themes:\n",
    "                            self.articles_data.append(\n",
    "                                {\n",
    "                                    \"url\": repo.html_url,\n",
    "                                    \"title\": repo.name,\n",
    "                                    \"text\": repo_text[:5000],  # Limit text length\n",
    "                                    \"summary\": repo.description or \"\",\n",
    "                                    \"publish_date\": repo.created_at.strftime(\n",
    "                                        \"%Y-%m-%d\"\n",
    "                                    ),\n",
    "                                    \"author\": repo.owner.login,\n",
    "                                    \"matched_themes\": matched_themes,\n",
    "                                    \"source\": \"github\",\n",
    "                                    \"stars\": repo.stargazers_count,\n",
    "                                    \"forks\": repo.forks_count,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(\n",
    "                        f\"Error processing repository {repo.full_name}: {str(e)}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error searching GitHub repositories: {str(e)}\")\n",
    "\n",
    "    def save_results(self, output_prefix=\"forum_analysis\"):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        output_dir = Path(\"results\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Create complete report\n",
    "        report = {\n",
    "            \"summary\": {\n",
    "                \"total_posts\": len(self.articles_data),\n",
    "                \"analysis_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"themes_analyzed\": list(self.themes.keys()),\n",
    "                \"debug_stats\": dict(self.debug_stats),\n",
    "            },\n",
    "            \"theme_analysis\": defaultdict(lambda: {\"posts\": [], \"count\": 0}),\n",
    "        }\n",
    "\n",
    "        # Organize posts by theme\n",
    "        for post in self.articles_data:\n",
    "            for theme in post[\"matched_themes\"].keys():\n",
    "                report[\"theme_analysis\"][theme][\"posts\"].append(\n",
    "                    {\n",
    "                        \"url\": post[\"url\"],\n",
    "                        \"title\": post[\"title\"],\n",
    "                        \"source\": post[\"source\"],\n",
    "                        \"publish_date\": post[\"publish_date\"],\n",
    "                    }\n",
    "                )\n",
    "                report[\"theme_analysis\"][theme][\"count\"] += 1\n",
    "\n",
    "        # Convert defaultdict to regular dict for JSON serialization\n",
    "        report[\"theme_analysis\"] = dict(report[\"theme_analysis\"])\n",
    "\n",
    "        # Save files\n",
    "        with open(\n",
    "            output_dir / f\"{output_prefix}_report.json\", \"w\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Create DataFrame for analysis\n",
    "        posts_df = pd.DataFrame(self.articles_data)\n",
    "        posts_df.to_csv(output_dir / f\"{output_prefix}_posts.csv\", index=False)\n",
    "\n",
    "        logging.info(\n",
    "            f\"Results saved in 'results' directory with prefix '{output_prefix}'\"\n",
    "        )\n",
    "        return report\n",
    "\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize analyzer\n",
    "    analyzer = ForumAnalyzer(\n",
    "        themes_csv_path=themes_csv_path,\n",
    "        reddit_credentials=reddit_credentials,\n",
    "        github_token=github_token,\n",
    "    )\n",
    "\n",
    "    # Analyze different platforms\n",
    "    analyzer.analyze_reddit()\n",
    "    analyzer.analyze_github()\n",
    "\n",
    "    # Save results\n",
    "    report = analyzer.save_results()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"Total posts analyzed: {report['summary']['total_posts']}\")\n",
    "    print(\"\\nResults by theme:\")\n",
    "    for theme, data in report[\"theme_analysis\"].items():\n",
    "        print(f\"\\n{theme}:\")\n",
    "        print(f\"  Posts: {data['count']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
