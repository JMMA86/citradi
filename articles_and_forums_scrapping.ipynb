{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install 'lxml[html_clean]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import newspaper\n",
    "from newspaper import Article, Config\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "class ThemeProcessor:\n",
    "    @staticmethod\n",
    "    def process_keywords_from_csv(csv_path):\n",
    "        \"\"\"Process keywords from CSV file\"\"\"\n",
    "            df = pd.read_csv(csv_path, header=None)  # Asumimos que no hay headers, si los hay, ajustar\n",
    "            themes_dict = defaultdict(list)\n",
    "\n",
    "            # Por cada columna en el DataFrame\n",
    "            for col in df.columns:\n",
    "                current_theme = None\n",
    "                for val in df[col].dropna():\n",
    "                    line = str(val).strip()\n",
    "\n",
    "                    # Si la línea inicia con '===' y finaliza con '===', es un nuevo tema\n",
    "                    if line.startswith('===') and line.endswith('==='):\n",
    "                        # Extraemos el nombre del tema quitando los '==='\n",
    "                        # Por ejemplo: === Inclusive growth, sustainable development and well-being ===\n",
    "                        # Queremos quedarnos solo con el texto interno\n",
    "                        theme_name = line.strip('=').strip()\n",
    "                        current_theme = theme_name\n",
    "                        if current_theme not in themes_dict:\n",
    "                            themes_dict[current_theme] = []\n",
    "                    \n",
    "                    # Si la línea empieza con '-', es un subtema asociado al tema actual\n",
    "                    elif line.startswith('-') and current_theme:\n",
    "                        subtopic = line.lstrip('-').strip()\n",
    "                        if subtopic:\n",
    "                            themes_dict[current_theme].append(subtopic)\n",
    "\n",
    "            # Remover duplicados en las listas (opcional)\n",
    "            for theme in themes_dict:\n",
    "                themes_dict[theme] = list(set(themes_dict[theme]))\n",
    "\n",
    "            return dict(themes_dict)\n",
    "\n",
    "class AINewsAnalyzer:\n",
    "    def __init__(self, themes_csv_path, news_sources=None):\n",
    "        \"\"\"\n",
    "        Initialize the AI News Analyzer with themes from CSV\n",
    "\n",
    "        Args:\n",
    "            themes_csv_path (str): Path to CSV file containing themes and keywords\n",
    "            news_sources (list): Optional list of news sources to analyze\n",
    "        \"\"\"\n",
    "        self.themes = ThemeProcessor.process_keywords_from_csv(themes_csv_path)\n",
    "        self.news_sources = news_sources or [\n",
    "            'https://efe.com/en/',\n",
    "            'http://www.wired.com',\n",
    "            'http://www.bbc.com',\n",
    "            'http://www.cnn.com',\n",
    "            'http://www.reuters.com',\n",
    "            'http://www.theguardian.com',\n",
    "            'http://www.nytimes.com',\n",
    "            'https://www.afp.com',\n",
    "            'https://www.wired.com',\n",
    "            'https://www.theguardian.com/technology',\n",
    "\n",
    "        ]\n",
    "\n",
    "        self.ai_related_terms = [\n",
    "            'artificial intelligence', 'AI', 'machine learning', 'deep learning',\n",
    "            'neural network', 'AI model', 'large language model', 'LLM',\n",
    "            'ChatGPT', 'GPT', 'artificial neural', 'AI system'\n",
    "        ]\n",
    "\n",
    "        self.articles_data = []\n",
    "        self.debug_stats = {\n",
    "            'total_urls_found': 0,\n",
    "            'download_failures': 0,\n",
    "            'parsing_failures': 0,\n",
    "            'ai_related_found': 0,\n",
    "            'theme_matched': 0\n",
    "        }\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        log_dir = Path('logs')\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / 'ai_news_analyzer.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def is_ai_related(self, text, title):\n",
    "        \"\"\"Check if article is AI-related and matches themes\"\"\"\n",
    "        combined_text = (text + \" \" + title).lower()\n",
    "\n",
    "        # First check if it's AI-related\n",
    "        if not any(term.lower() in combined_text for term in self.ai_related_terms):\n",
    "            return False\n",
    "\n",
    "        # Then check if it matches any of our theme keywords\n",
    "        for theme, keywords in self.themes.items():\n",
    "            if any(keyword.lower() in combined_text for keyword in keywords):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def download_and_parse_article(self, article_url):\n",
    "        \"\"\"Download and parse a single article with better error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nAttempting to process: {article_url}\")  # Debug print\n",
    "\n",
    "            article = Article(article_url)\n",
    "            try:\n",
    "                article.download()\n",
    "                time.sleep(1)  # Increased delay to be more polite\n",
    "            except Exception as e:\n",
    "                self.debug_stats['download_failures'] += 1\n",
    "                print(f\"Download failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                article.parse()\n",
    "                article.nlp()\n",
    "            except Exception as e:\n",
    "                self.debug_stats['parsing_failures'] += 1\n",
    "                print(f\"Parsing failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "            # Check if we got actual content\n",
    "            if not article.text or len(article.text) < 100:\n",
    "                print(\"Article too short or empty\")\n",
    "                return None\n",
    "\n",
    "            if self.is_ai_related(article.text, article.title):\n",
    "                self.debug_stats['ai_related_found'] += 1\n",
    "                print(\"AI-related article found!\")\n",
    "\n",
    "                # Match themes and keywords\n",
    "                matched_themes = {}\n",
    "                text = (article.text + \" \" + article.title).lower()\n",
    "\n",
    "                for theme, keywords in self.themes.items():\n",
    "                    matched_keywords = []\n",
    "                    for keyword in keywords:\n",
    "                        if keyword.lower() in text:\n",
    "                            matched_keywords.append(keyword)\n",
    "                    if matched_keywords:\n",
    "                        matched_themes[theme] = matched_keywords\n",
    "\n",
    "                if matched_themes:\n",
    "                    self.debug_stats['theme_matched'] += 1\n",
    "                    return {\n",
    "                        'url': article_url,\n",
    "                        'title': article.title,\n",
    "                        'text': article.text,\n",
    "                        'summary': article.summary,\n",
    "                        'keywords': article.keywords,\n",
    "                        'publish_date': article.publish_date.strftime('%Y-%m-%d') if article.publish_date else None,\n",
    "                        'authors': article.authors,\n",
    "                        'matched_themes': matched_themes,\n",
    "                        'source': re.findall(r'https?://(?:www\\.)?([^/]+)', article_url)[0]\n",
    "                    }\n",
    "            else:\n",
    "                print(\"Not AI-related\")\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {article_url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sources(self, max_articles_per_source=50, start_year=2014):\n",
    "        \"\"\"Analyze news sources with better debugging\"\"\"\n",
    "        for source_url in tqdm(self.news_sources, desc=\"Processing news sources\"):\n",
    "            try:\n",
    "                print(f\"\\nProcessing source: {source_url}\")\n",
    "\n",
    "                config = Config()\n",
    "                config.request_timeout = 30  # Increased timeout\n",
    "                config.memoize_articles = False\n",
    "                config.fetch_images = False\n",
    "\n",
    "                # Build source object\n",
    "                source = newspaper.build(\n",
    "                    source_url,\n",
    "                    config=config,\n",
    "                    language='en',\n",
    "                    number_threads=1\n",
    "                )\n",
    "\n",
    "                print(f\"Found {len(source.articles)} articles at {source_url}\")\n",
    "                self.debug_stats['total_urls_found'] += len(source.articles)\n",
    "\n",
    "                # Get article URLs\n",
    "                ai_related_urls = []\n",
    "                for article in tqdm(source.articles[:max_articles_per_source * 2],\n",
    "                                  desc=f\"Scanning articles from {source_url}\"):\n",
    "                    if not article.url:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "\n",
    "                        if not article.text or len(article.text) < 100:\n",
    "                            continue\n",
    "\n",
    "                        if article.publish_date:\n",
    "                            article_year = article.publish_date.year\n",
    "                            if start_year <= article_year <= datetime.now().year:\n",
    "                                if self.is_ai_related(article.text, article.title):\n",
    "                                    ai_related_urls.append(article.url)\n",
    "                                    print(f\"Found AI article: {article.url}\")\n",
    "\n",
    "                        if len(ai_related_urls) >= max_articles_per_source:\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing article: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"Found {len(ai_related_urls)} AI-related articles\")\n",
    "\n",
    "                # Process the found articles\n",
    "                with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "                    results = list(executor.map(self.download_and_parse_article, ai_related_urls))\n",
    "\n",
    "                valid_results = [r for r in results if r is not None]\n",
    "                self.articles_data.extend(valid_results)\n",
    "\n",
    "                print(f\"Successfully processed {len(valid_results)} articles\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing source {source_url}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def analyze_content(self):\n",
    "        \"\"\"Analyze collected articles for themes\"\"\"\n",
    "        analysis_results = defaultdict(lambda: defaultdict(int))\n",
    "        articles_by_theme = defaultdict(list)\n",
    "\n",
    "        for article in tqdm(self.articles_data, desc=\"Analyzing articles\"):\n",
    "            for theme, keywords in article['matched_themes'].items():\n",
    "                analysis_results[theme]['articles_count'] += 1\n",
    "                analysis_results[theme]['keyword_occurrences'] += len(keywords)\n",
    "\n",
    "                articles_by_theme[theme].append({\n",
    "                    'url': article['url'],\n",
    "                    'title': article['title'],\n",
    "                    'publish_date': article['publish_date'],\n",
    "                    'keywords_found': keywords\n",
    "                })\n",
    "\n",
    "        return analysis_results, articles_by_theme\n",
    "\n",
    "    def save_results(self, analysis_results, articles_by_theme, output_prefix='ai_news_analysis'):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        # Ensure output directory exists\n",
    "        output_dir = Path('results')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Create complete report\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_articles': len(self.articles_data),\n",
    "                'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'themes_analyzed': list(self.themes.keys())\n",
    "            },\n",
    "            'theme_analysis': {\n",
    "                theme: {\n",
    "                    'articles_count': data['articles_count'],\n",
    "                    'keyword_occurrences': data['keyword_occurrences'],\n",
    "                    'articles': articles_by_theme[theme]\n",
    "                }\n",
    "                for theme, data in analysis_results.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save files\n",
    "        with open(output_dir / f'{output_prefix}_report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Create theme analysis DataFrame\n",
    "        theme_df = pd.DataFrame([\n",
    "            {\n",
    "                'theme': theme,\n",
    "                'articles_count': data['articles_count'],\n",
    "                'keyword_occurrences': data['keyword_occurrences'],\n",
    "                'percentage_of_total': (data['articles_count'] / len(self.articles_data) * 100) if self.articles_data else 0\n",
    "            }\n",
    "            for theme, data in analysis_results.items()\n",
    "        ])\n",
    "        theme_df.to_csv(output_dir / f'{output_prefix}_theme_analysis.csv', index=False)\n",
    "\n",
    "        # Create articles DataFrame\n",
    "        articles_df = pd.DataFrame([\n",
    "            {\n",
    "                'theme': theme,\n",
    "                'title': article['title'],\n",
    "                'url': article['url'],\n",
    "                'publish_date': article['publish_date'],\n",
    "                'keywords': ', '.join(article['keywords_found'])\n",
    "            }\n",
    "            for theme, articles in articles_by_theme.items()\n",
    "            for article in articles\n",
    "        ])\n",
    "        articles_df.to_csv(output_dir / f'{output_prefix}_articles.csv', index=False)\n",
    "\n",
    "        logging.info(f\"Results saved in 'results' directory with prefix '{output_prefix}'\")\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize analyzer with your CSV file\n",
    "    analyzer = AINewsAnalyzer('Criterios.csv')\n",
    "\n",
    "    # Analyze sources\n",
    "    analyzer.analyze_sources(max_articles_per_source=100, start_year=2014)\n",
    "\n",
    "    # Analyze content\n",
    "    analysis_results, articles_by_theme = analyzer.analyze_content()\n",
    "\n",
    "    # Save and get report\n",
    "    report = analyzer.save_results(analysis_results, articles_by_theme)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"Total articles analyzed: {report['summary']['total_articles']}\")\n",
    "    print(\"\\nResults by theme:\")\n",
    "    for theme, data in report['theme_analysis'].items():\n",
    "        print(f\"\\n{theme}:\")\n",
    "        print(f\"  Articles: {data['articles_count']}\")\n",
    "        print(f\"  Keyword occurrences: {data['keyword_occurrences']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ThemeProcessor:\n",
    "    @staticmethod\n",
    "    def process_keywords_from_csv(csv_path):\n",
    "        \"\"\"Process keywords from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_path, header=None)  # Asumimos que no hay headers, si los hay, ajustar\n",
    "            themes_dict = defaultdict(list)\n",
    "\n",
    "            # Por cada columna en el DataFrame\n",
    "            for col in df.columns:\n",
    "                current_theme = None\n",
    "                for val in df[col].dropna():\n",
    "                    line = str(val).strip()\n",
    "\n",
    "                    # Si la línea inicia con '===' y finaliza con '===', es un nuevo tema\n",
    "                    if line.startswith('===') and line.endswith('==='):\n",
    "                        # Extraemos el nombre del tema quitando los '==='\n",
    "                        # Por ejemplo: === Inclusive growth, sustainable development and well-being ===\n",
    "                        # Queremos quedarnos solo con el texto interno\n",
    "                        theme_name = line.strip('=').strip()\n",
    "                        current_theme = theme_name\n",
    "                        if current_theme not in themes_dict:\n",
    "                            themes_dict[current_theme] = []\n",
    "                    \n",
    "                    # Si la línea empieza con '-', es un subtema asociado al tema actual\n",
    "                    elif line.startswith('-') and current_theme:\n",
    "                        subtopic = line.lstrip('-').strip()\n",
    "                        if subtopic:\n",
    "                            themes_dict[current_theme].append(subtopic)\n",
    "\n",
    "            # Remover duplicados en las listas (opcional)\n",
    "            for theme in themes_dict:\n",
    "                themes_dict[theme] = list(set(themes_dict[theme]))\n",
    "\n",
    "            return dict(themes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from github import Github\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "\n",
    "class ForumAnalyzer:\n",
    "    def __init__(self, themes_csv_path, reddit_credentials=None, github_token=None):\n",
    "        \"\"\"\n",
    "        Initialize the Forum Analyzer\n",
    "\n",
    "        Args:\n",
    "            themes_csv_path (str): Path to CSV file containing themes and keywords\n",
    "            reddit_credentials (dict): Dictionary with Reddit API credentials\n",
    "            github_token (str): GitHub personal access token\n",
    "        \"\"\"\n",
    "        self.themes = ThemeProcessor.process_keywords_from_csv(themes_csv_path)\n",
    "        self.articles_data = []\n",
    "\n",
    "        # Initialize Reddit client if credentials provided\n",
    "        self.reddit = None\n",
    "        if reddit_credentials:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=reddit_credentials['client_id'],\n",
    "                client_secret=reddit_credentials['client_secret'],\n",
    "                user_agent=reddit_credentials['user_agent']\n",
    "            )\n",
    "\n",
    "        # Initialize GitHub client if token provided\n",
    "        self.github = None\n",
    "        if github_token:\n",
    "            self.github = Github(github_token)\n",
    "\n",
    "        # Initialize logging\n",
    "        self.setup_logging()\n",
    "\n",
    "        # AI-related terms (inherited from AINewsAnalyzer)\n",
    "        self.ai_related_terms = [\n",
    "            'artificial intelligence', 'AI', 'machine learning', 'deep learning',\n",
    "            'neural network', 'AI model', 'large language model', 'LLM',\n",
    "            'ChatGPT', 'GPT', 'artificial neural', 'AI system'\n",
    "        ]\n",
    "\n",
    "        # Debug stats\n",
    "        self.debug_stats = defaultdict(int)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        log_dir = Path('logs')\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / 'forum_analyzer.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def is_ai_related(self, text, title=\"\"):\n",
    "        \"\"\"Check if content is AI-related and matches themes\"\"\"\n",
    "        combined_text = (text + \" \" + title).lower()\n",
    "\n",
    "        # First check if it's AI-related\n",
    "        if not any(term.lower() in combined_text for term in self.ai_related_terms):\n",
    "            return False\n",
    "\n",
    "        # Then check if it matches any of our theme keywords\n",
    "        for theme, keywords in self.themes.items():\n",
    "            if any(keyword.lower() in combined_text for keyword in keywords):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def analyze_reddit(self, subreddits=['artificial', 'MachineLearning'],\n",
    "                      time_filter='year', limit=1000):\n",
    "        \"\"\"\n",
    "        Analyze Reddit posts from specified subreddits\n",
    "\n",
    "        Args:\n",
    "            subreddits (list): List of subreddit names to analyze\n",
    "            time_filter (str): One of 'day', 'week', 'month', 'year', 'all'\n",
    "            limit (int): Maximum number of posts to analyze per subreddit\n",
    "        \"\"\"\n",
    "        if not self.reddit:\n",
    "            logging.error(\"Reddit client not initialized. Please provide credentials.\")\n",
    "            return\n",
    "\n",
    "        for subreddit_name in tqdm(subreddits, desc=\"Processing subreddits\"):\n",
    "            try:\n",
    "                subreddit = self.reddit.subreddit(subreddit_name)\n",
    "\n",
    "                # Get top posts\n",
    "                for post in tqdm(subreddit.top(time_filter=time_filter, limit=limit),\n",
    "                               desc=f\"Analyzing posts from r/{subreddit_name}\"):\n",
    "\n",
    "                    # Combine post title, content and top comments\n",
    "                    post_text = f\"{post.title} {post.selftext}\"\n",
    "\n",
    "                    # Add top comments\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    comments_text = \" \".join([comment.body for comment in post.comments.list()[:10]])\n",
    "\n",
    "                    combined_text = post_text + \" \" + comments_text\n",
    "\n",
    "                    if self.is_ai_related(combined_text, post.title):\n",
    "                        self.debug_stats['reddit_ai_related'] += 1\n",
    "\n",
    "                        # Match themes and keywords\n",
    "                        matched_themes = {}\n",
    "                        text = combined_text.lower()\n",
    "\n",
    "                        for theme, keywords in self.themes.items():\n",
    "                            matched_keywords = [k for k in keywords if k.lower() in text]\n",
    "                            if matched_keywords:\n",
    "                                matched_themes[theme] = matched_keywords\n",
    "\n",
    "                        if matched_themes:\n",
    "                            self.articles_data.append({\n",
    "                                'url': f\"https://reddit.com{post.permalink}\",\n",
    "                                'title': post.title,\n",
    "                                'text': combined_text,\n",
    "                                'summary': post.selftext[:500] if post.selftext else \"\",\n",
    "                                'publish_date': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d'),\n",
    "                                'author': str(post.author),\n",
    "                                'matched_themes': matched_themes,\n",
    "                                'source': f\"reddit/r/{subreddit_name}\",\n",
    "                                'score': post.score,\n",
    "                                'num_comments': post.num_comments\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing subreddit {subreddit_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def analyze_github(self, query='artificial intelligence', sort='stars',\n",
    "                      max_repos=100, min_stars=100):\n",
    "        \"\"\"\n",
    "        Analyze GitHub repositories and their discussions\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query for repositories\n",
    "            sort (str): How to sort results ('stars', 'forks', 'updated')\n",
    "            max_repos (int): Maximum number of repositories to analyze\n",
    "            min_stars (int): Minimum number of stars for a repository\n",
    "        \"\"\"\n",
    "        if not self.github:\n",
    "            logging.error(\"GitHub client not initialized. Please provide token.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Search repositories\n",
    "            repositories = self.github.search_repositories(\n",
    "                query=f\"{query} stars:>={min_stars}\",\n",
    "                sort=sort,\n",
    "                order='desc'\n",
    "            )\n",
    "\n",
    "            for repo in tqdm(repositories[:max_repos], desc=\"Analyzing GitHub repositories\"):\n",
    "                try:\n",
    "                    # Combine repository description, readme, and discussions\n",
    "                    repo_text = f\"{repo.description or ''}\"\n",
    "\n",
    "                    try:\n",
    "                        readme = repo.get_readme().decoded_content.decode()\n",
    "                        repo_text += \" \" + readme\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Get discussions if available\n",
    "                    if repo.has_discussions:\n",
    "                        discussions = repo.get_discussions()\n",
    "                        for discussion in discussions[:10]:  # Get first 10 discussions\n",
    "                            repo_text += f\" {discussion.title} {discussion.body}\"\n",
    "\n",
    "                    if self.is_ai_related(repo_text, repo.name):\n",
    "                        self.debug_stats['github_ai_related'] += 1\n",
    "\n",
    "                        # Match themes and keywords\n",
    "                        matched_themes = {}\n",
    "                        text = repo_text.lower()\n",
    "\n",
    "                        for theme, keywords in self.themes.items():\n",
    "                            matched_keywords = [k for k in keywords if k.lower() in text]\n",
    "                            if matched_keywords:\n",
    "                                matched_themes[theme] = matched_keywords\n",
    "\n",
    "                        if matched_themes:\n",
    "                            self.articles_data.append({\n",
    "                                'url': repo.html_url,\n",
    "                                'title': repo.name,\n",
    "                                'text': repo_text[:5000],  # Limit text length\n",
    "                                'summary': repo.description or \"\",\n",
    "                                'publish_date': repo.created_at.strftime('%Y-%m-%d'),\n",
    "                                'author': repo.owner.login,\n",
    "                                'matched_themes': matched_themes,\n",
    "                                'source': 'github',\n",
    "                                'stars': repo.stargazers_count,\n",
    "                                'forks': repo.forks_count\n",
    "                            })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing repository {repo.full_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error searching GitHub repositories: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_results(self, output_prefix='forum_analysis'):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        output_dir = Path('results')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Create complete report\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_posts': len(self.articles_data),\n",
    "                'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'themes_analyzed': list(self.themes.keys()),\n",
    "                'debug_stats': dict(self.debug_stats)\n",
    "            },\n",
    "            'theme_analysis': defaultdict(lambda: {'posts': [], 'count': 0})\n",
    "        }\n",
    "\n",
    "        # Organize posts by theme\n",
    "        for post in self.articles_data:\n",
    "            for theme in post['matched_themes'].keys():\n",
    "                report['theme_analysis'][theme]['posts'].append({\n",
    "                    'url': post['url'],\n",
    "                    'title': post['title'],\n",
    "                    'source': post['source'],\n",
    "                    'publish_date': post['publish_date']\n",
    "                })\n",
    "                report['theme_analysis'][theme]['count'] += 1\n",
    "\n",
    "        # Convert defaultdict to regular dict for JSON serialization\n",
    "        report['theme_analysis'] = dict(report['theme_analysis'])\n",
    "\n",
    "        # Save files\n",
    "        with open(output_dir / f'{output_prefix}_report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Create DataFrame for analysis\n",
    "        posts_df = pd.DataFrame(self.articles_data)\n",
    "        posts_df.to_csv(output_dir / f'{output_prefix}_posts.csv', index=False)\n",
    "\n",
    "        logging.info(f\"Results saved in 'results' directory with prefix '{output_prefix}'\")\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Reddit API credentials (you'll need to get these from Reddit)\n",
    "    reddit_credentials = {\n",
    "        'client_id': 'vopzVv6U6FpL5xZXQlcifA',\n",
    "        'client_secret': 'rrY4l3SyjVWWFF8Tfx4hJSGTFyVb8A',\n",
    "        'user_agent': 'python:ai_forum_analyzer:v1.0:Investigacion'\n",
    "    }\n",
    "\n",
    "    # GitHub personal access token (you'll need to create this)\n",
    "    github_token = ''\n",
    "\n",
    "    # Initialize analyzer\n",
    "    analyzer = ForumAnalyzer(\n",
    "        'Criterios.csv',\n",
    "        reddit_credentials=reddit_credentials,\n",
    "        github_token=github_token\n",
    "    )\n",
    "\n",
    "    # Analyze different platforms\n",
    "    analyzer.analyze_reddit()\n",
    "    analyzer.analyze_github()\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    report = analyzer.save_results()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"Total posts analyzed: {report['summary']['total_posts']}\")\n",
    "    print(\"\\nResults by theme:\")\n",
    "    for theme, data in report['theme_analysis'].items():\n",
    "        print(f\"\\n{theme}:\")\n",
    "        print(f\"  Posts: {data['count']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
