{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: lxml[html_clean] in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: lxml_html_clean in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from lxml[html_clean]) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: newspaper3k in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (4.13.4)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (11.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (1.3.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (6.0.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (2.32.4)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (5.3.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.1)\n",
      "Requirement already satisfied: six in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2025.8.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.19.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: newspaper3k in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (4.13.4)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (11.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (1.3.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (6.0.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (2.32.4)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (5.3.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.1)\n",
      "Requirement already satisfied: six in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2025.8.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.19.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: praw in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: praw in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: PyGithub in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from PyGithub) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.14.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from PyGithub) (2.32.4)\n",
      "Requirement already satisfied: pyjwt>=2.4.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from PyGithub) (4.14.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from PyGithub) (2.5.0)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (45.0.6)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.4.0->PyGithub) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from cffi>=1.14->cryptography>=3.4.0->pyjwt[crypto]>=2.4.0->PyGithub) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.14.0->PyGithub) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: PyGithub in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from PyGithub) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.14.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from PyGithub) (2.32.4)\n",
      "Requirement already satisfied: pyjwt>=2.4.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from PyGithub) (4.14.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from PyGithub) (2.5.0)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (45.0.6)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.4.0->PyGithub) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from cffi>=1.14->cryptography>=3.4.0->pyjwt[crypto]>=2.4.0->PyGithub) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from requests>=2.14.0->PyGithub) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gagig\\downloads\\citradi\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml[html_clean]\n",
    "%pip install newspaper3k\n",
    "%pip install praw\n",
    "%pip install PyGithub\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from github import Github\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "import newspaper\n",
    "from newspaper import Article, Config\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import praw\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThemeProcessor:\n",
    "    @staticmethod\n",
    "    def process_keywords_from_csv(csv_path):\n",
    "        \"\"\"Process keywords from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_path, sep=';', header=None)  # Asumimos que no hay headers, si los hay, ajustar\n",
    "        themes_dict = defaultdict(list)\n",
    "\n",
    "        # Por cada columna en el DataFrame\n",
    "        for col in df.columns:\n",
    "            current_theme = None\n",
    "            for val in df[col].dropna():\n",
    "                line = str(val).strip()\n",
    "\n",
    "                # Si la línea inicia con '===' y finaliza con '===', es un nuevo tema\n",
    "                if line.startswith('===') and line.endswith('==='):\n",
    "                    # Extraemos el nombre del tema quitando los '==='\n",
    "                    # Por ejemplo: === Inclusive growth, sustainable development and well-being ===\n",
    "                    # Queremos quedarnos solo con el texto interno\n",
    "                    theme_name = line.strip('=').strip()\n",
    "                    current_theme = theme_name\n",
    "                    if current_theme not in themes_dict:\n",
    "                        themes_dict[current_theme] = []\n",
    "                \n",
    "                # Si la línea empieza con '-', es un subtema asociado al tema actual\n",
    "                elif line.startswith('-') and current_theme:\n",
    "                    subtopic = line.lstrip('-').strip()\n",
    "                    if subtopic:\n",
    "                        themes_dict[current_theme].append(subtopic)\n",
    "\n",
    "        # Remover duplicados en las listas (opcional)\n",
    "        for theme in themes_dict:\n",
    "            themes_dict[theme] = list(set(themes_dict[theme]))\n",
    "\n",
    "        return dict(themes_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIGen Keywords\n",
    "globalKeywords = [\n",
    "    \"AI\",\n",
    "    \"AI content detection\",\n",
    "    \"AI-generated audio\",\n",
    "    \"AI-generated content\",\n",
    "    \"AI-generated content detection\",\n",
    "    \"AI-generated images\",\n",
    "    \"AI-generated text\",\n",
    "    \"AI-generated video\",\n",
    "    \"AI model\",\n",
    "    \"AI models\",\n",
    "    \"artificial intelligence\",\n",
    "    \"artificial intelligence model\",\n",
    "    \"artificial intelligence models\",\n",
    "    \"attention model\",\n",
    "    \"automation\",\n",
    "    \"BERT\",\n",
    "    \"GPT-style\",\n",
    "    \"BERT/GPT-style model\",\n",
    "    \"conversational models\",\n",
    "    \"creative AI\",\n",
    "    \"deep learning model\",\n",
    "    \"deep learning models\",\n",
    "    \"deepfakes\",\n",
    "    \"generation models\",\n",
    "    \"generative AI\",\n",
    "    \"language model\",\n",
    "    \"language models\",\n",
    "    \"large language model\",\n",
    "    \"large language models\",\n",
    "    \"LLM\",\n",
    "    \"LLMs\",\n",
    "    \"machine-generated content\",\n",
    "    \"machine learning\",\n",
    "    \"neural language model\",\n",
    "    \"neural language models\",\n",
    "    \"synthetic media\",\n",
    "    \"virtual agents\",\n",
    "]\n",
    "# DeepSeek Keywords\n",
    "# globalKeywords = [\n",
    "#     'Deep Seek',\n",
    "#     'DeepSeek',\n",
    "#     \"DeepSeek's\",\n",
    "#     'DeepSeek LLM',\n",
    "#     'DeepSeek 1',\n",
    "#     'DeepSeek 2',\n",
    "#     'DeepSeek-V2',\n",
    "#     'DeepSeek-Coder',\n",
    "#     'DeepSeek-Math',\n",
    "#     'DeepSeek Chat',\n",
    "#     'DeepSeek AI',\n",
    "#     'DeepSeek model',\n",
    "#     'DeepSeek models',\n",
    "#     'DeepSeek API',\n",
    "#     'DeepSeek for coding',\n",
    "#     'DeepSeek code generation',\n",
    "#     'Chinese LLM',\n",
    "#     'open-source Chinese AI',\n",
    "#     'large language model China',\n",
    "#     'bilingual AI model',\n",
    "#     'AI model for developers'\n",
    "# ]\n",
    "# OpenAI Keywords\n",
    "# globalKeywords = [\n",
    "#     \"Chat GPT\",\n",
    "#     \"ChatGPT\",\n",
    "#     \"chatgpt\",\n",
    "#     \"GPT\",\n",
    "#     \"GPT-3\",\n",
    "#     \"GPT-4\",\n",
    "#     \"gpt 3.5\",\n",
    "#     \"gpt-4o\",\n",
    "#     \"GPT model\",\n",
    "#     \"GPT-models\",\n",
    "#     \"gpt models\",\n",
    "#     \"GPT-powered assistant\",\n",
    "#     \"Open AI\",\n",
    "#     \"Open AI's\",\n",
    "#     \"OpenAI\",\n",
    "#     \"OpenAI Codex\",\n",
    "#     \"OpenAi DALL·E\",\n",
    "#     \"OpenAI Whisper\"\n",
    "# ]\n",
    "# Google Keywords\n",
    "# globalKeywords = [\n",
    "#     \"Google A.I.\",\n",
    "#     \"Google AI\",\n",
    "#     \"Google AI models\",\n",
    "#     \"Google AI tools\",\n",
    "#     \"Google Ai\",\n",
    "#     \"Google AI\",\n",
    "#     \"Google Bard\",\n",
    "#     \"Google Deepmind\",\n",
    "#     \"Google Gemini\",\n",
    "#     \"Google language model\",\n",
    "#     \"Google LLM\",\n",
    "#     \"Google LLMs\",\n",
    "#     \"Google models\",\n",
    "#     \"Google’s AI\",\n",
    "#     \"Google’s artificial intelligence\",\n",
    "#     \"Google’s LLMs\",\n",
    "#     \"Google’s models\",\n",
    "#     \"Google's AI\",\n",
    "#     \"Google's language model\",\n",
    "#     \"GoogleAI\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing news sources:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing source: https://efe.com/en/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 10:43:21,784 - CRITICAL - [REQUEST FAILED] 500 Server Error: Internal Server Error for url: https://efe.com/feed/\n",
      "2025-08-18 10:43:23,621 - CRITICAL - [REQUEST FAILED] 500 Server Error: Internal Server Error for url: https://efe.com/feed/\n",
      "2025-08-18 10:43:23,621 - CRITICAL - [REQUEST FAILED] 500 Server Error: Internal Server Error for url: https://efe.com/feed/\n",
      "2025-08-18 10:43:25,780 - CRITICAL - [REQUEST FAILED] 500 Server Error: Internal Server Error for url: https://efe.com/feed/\n",
      "2025-08-18 10:43:25,780 - CRITICAL - [REQUEST FAILED] 500 Server Error: Internal Server Error for url: https://efe.com/feed/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 articles at https://efe.com/en/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from https://efe.com/en/:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: https://efe.com/en/other-news/2025-08-14/trump-plays-his-cards-with-nvidia-and-intel-gpt-5-makes-its-debut/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from https://efe.com/en/: 100%|██████████| 19/19 [00:25<00:00,  1.33s/it][A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 AI-related articles\n",
      "\n",
      "Attempting to process: https://efe.com/en/other-news/2025-08-14/trump-plays-his-cards-with-nvidia-and-intel-gpt-5-makes-its-debut/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing news sources:  10%|█         | 1/10 [00:36<05:32, 36.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-related article found!\n",
      "Successfully processed 1 articles\n",
      "\n",
      "Processing source: http://www.wired.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 10:44:04,284 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.wired.com/feeds\n",
      "2025-08-18 10:44:04,450 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.wired.com/rss\n",
      "2025-08-18 10:44:04,450 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.wired.com/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 366 articles at http://www.wired.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: http://www.wired.com/story/photo-essay-school-tech-hysteria/\n",
      "Error processing article: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wired.com/v2/offers/wir_edit_hardcoded?source=Site_0_HCL_WIR_EDIT_HARDCODED_HOMEPAGE_MODULE_0_GLOBAL_JULY_2025_NEW_OFFER_ZZ on URL https://www.wired.com/v2/offers/wir_edit_hardcoded?source=Site_0_HCL_WIR_EDIT_HARDCODED_HOMEPAGE_MODULE_0_GLOBAL_JULY_2025_NEW_OFFER_ZZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:   8%|▊         | 17/200 [00:07<00:54,  3.33it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: http://www.wired.com/story/nvidia-chips-export-controls-trump-h20-security/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:   9%|▉         | 18/200 [00:07<00:57,  3.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: http://www.wired.com/story/gear-news-of-the-week-a-new-privacy-phone-arrives-and-samsung-has-a-115-inch-micro-rgb-tv/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  18%|█▊        | 36/200 [00:13<00:53,  3.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: http://www.wired.com/story/ai-slop-is-ripping-off-one-of-summers-best-games-fighting-back-is-harder-than-you-think/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  21%|██        | 42/200 [00:15<00:55,  2.85it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: http://www.wired.com/story/netflix-best-movies-this-week/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  22%|██▏       | 43/200 [00:15<00:58,  2.69it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: http://www.wired.com/story/netflix-best-shows-this-week/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  22%|██▏       | 44/200 [00:16<01:06,  2.33it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: http://www.wired.com/story/big-interview-bryan-johnson/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  26%|██▌       | 52/200 [00:19<01:00,  2.45it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/big-interview on URL http://www.wired.com/video/series/big-interview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  36%|███▌      | 72/200 [00:33<01:33,  1.37it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/incognito-mode on URL http://www.wired.com/video/series/incognito-mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  37%|███▋      | 74/200 [00:35<01:33,  1.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/wired-s-50-most-searched-questions on URL http://www.wired.com/video/series/wired-s-50-most-searched-questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  39%|███▉      | 78/200 [00:37<01:10,  1.73it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: https://www.wired.com/story/photo-essay-school-tech-hysteria/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  69%|██████▉   | 138/200 [01:43<00:14,  4.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: https://www.wired.com/story/nvidia-chips-export-controls-trump-h20-security/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  70%|██████▉   | 139/200 [01:44<00:13,  4.54it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: https://www.wired.com/story/gear-news-of-the-week-a-new-privacy-phone-arrives-and-samsung-has-a-115-inch-micro-rgb-tv/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  78%|███████▊  | 157/200 [01:47<00:09,  4.75it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: https://www.wired.com/story/ai-slop-is-ripping-off-one-of-summers-best-games-fighting-back-is-harder-than-you-think/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  82%|████████▏ | 163/200 [01:49<00:10,  3.57it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: https://www.wired.com/story/netflix-best-movies-this-week/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  82%|████████▏ | 164/200 [01:49<00:10,  3.39it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: https://www.wired.com/story/netflix-best-shows-this-week/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  82%|████████▎ | 165/200 [01:50<00:12,  2.90it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: https://www.wired.com/story/big-interview-bryan-johnson/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com:  86%|████████▋ | 173/200 [01:52<00:06,  4.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/big-interview on URL https://www.wired.com/video/series/big-interview\n",
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/incognito-mode on URL https://www.wired.com/video/series/incognito-mode\n",
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/wired-s-50-most-searched-questions on URL https://www.wired.com/video/series/wired-s-50-most-searched-questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com: 100%|██████████| 200/200 [02:00<00:00,  1.66it/s]\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 AI-related articles\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/photo-essay-school-tech-hysteria/\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/nvidia-chips-export-controls-trump-h20-security/\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/gear-news-of-the-week-a-new-privacy-phone-arrives-and-samsung-has-a-115-inch-micro-rgb-tv/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/ai-slop-is-ripping-off-one-of-summers-best-games-fighting-back-is-harder-than-you-think/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/netflix-best-movies-this-week/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/netflix-best-shows-this-week/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/ai-slop-is-ripping-off-one-of-summers-best-games-fighting-back-is-harder-than-you-think/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/netflix-best-movies-this-week/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/netflix-best-shows-this-week/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/big-interview-bryan-johnson/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: http://www.wired.com/story/big-interview-bryan-johnson/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/photo-essay-school-tech-hysteria/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/nvidia-chips-export-controls-trump-h20-security/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/photo-essay-school-tech-hysteria/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/nvidia-chips-export-controls-trump-h20-security/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/gear-news-of-the-week-a-new-privacy-phone-arrives-and-samsung-has-a-115-inch-micro-rgb-tv/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/gear-news-of-the-week-a-new-privacy-phone-arrives-and-samsung-has-a-115-inch-micro-rgb-tv/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/ai-slop-is-ripping-off-one-of-summers-best-games-fighting-back-is-harder-than-you-think/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/netflix-best-movies-this-week/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/ai-slop-is-ripping-off-one-of-summers-best-games-fighting-back-is-harder-than-you-think/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/netflix-best-movies-this-week/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/netflix-best-shows-this-week/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/netflix-best-shows-this-week/\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/big-interview-bryan-johnson/\n",
      "AI-related article found!\n",
      "AI-related article found!\n",
      "\n",
      "Attempting to process: https://www.wired.com/story/big-interview-bryan-johnson/\n",
      "AI-related article found!\n",
      "AI-related article found!\n",
      "AI-related article found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing news sources:  20%|██        | 2/10 [02:57<13:01, 97.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-related article found!\n",
      "Successfully processed 14 articles\n",
      "\n",
      "Processing source: http://www.bbc.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 10:46:18,572 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.bbc.com/undefined\n",
      "2025-08-18 10:47:54,389 - CRITICAL - [REQUEST FAILED] HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=30)\n",
      "2025-08-18 10:47:54,389 - CRITICAL - [REQUEST FAILED] HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=30)\n",
      "2025-08-18 10:47:57,055 - WARNING - Deleting category http://www.bbc.com/live from source http://www.bbc.com due to download error\n",
      "2025-08-18 10:47:57,055 - WARNING - Deleting category http://www.bbc.com/live from source http://www.bbc.com due to download error\n",
      "2025-08-18 10:48:05,765 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.bbc.com/feed\n",
      "2025-08-18 10:48:05,765 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.bbc.com/feed\n",
      "2025-08-18 10:48:13,594 - CRITICAL - [REQUEST FAILED] Exceeded 30 redirects.\n",
      "2025-08-18 10:48:13,594 - CRITICAL - [REQUEST FAILED] Exceeded 30 redirects.\n",
      "2025-08-18 10:48:16,152 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.bbc.com/rss\n",
      "2025-08-18 10:48:16,152 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.bbc.com/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 545 articles at http://www.bbc.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /news/northern_ireland (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000029E3D54D370>, 'Connection to www.bbc.com timed out. (connect timeout=30)')) on URL http://www.bbc.com/news/northern_ireland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:   4%|▍         | 8/200 [00:35<20:17,  6.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /news/world/australia (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000029E3D77D610>, 'Connection to www.bbc.com timed out. (connect timeout=30)')) on URL http://www.bbc.com/news/world/australia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:   9%|▉         | 18/200 [01:44<22:08,  7.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /news/articles/cewy88jle0eo (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000029E3D7468D0>, 'Connection to www.bbc.com timed out. (connect timeout=30)')) on URL http://www.bbc.com/news/articles/cewy88jle0eo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:  14%|█▍        | 29/200 [02:33<17:11,  6.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=30) on URL http://www.bbc.com/news/videos/c3wn7v0gxy4o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:  23%|██▎       | 46/200 [03:45<22:16,  8.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found AI article: http://www.bbc.com/future/article/20250616-how-bibliotherapy-can-both-help-and-harm-your-mental-health\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:  26%|██▋       | 53/200 [04:10<08:43,  3.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=30) on URL http://www.bbc.com/news/articles/c4g0jnw7v8jo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:  34%|███▍      | 69/200 [06:05<24:18, 11.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=30) on URL http://www.bbc.com/news/articles/cp94jz0y7ygo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:  40%|████      | 81/200 [07:08<20:42, 10.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /news/world/us_and_canada (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000029E3D5D67B0>, 'Connection to www.bbc.com timed out. (connect timeout=30)')) on URL http://www.bbc.com/news/world/us_and_canada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com:  52%|█████▏    | 104/200 [08:56<11:36,  7.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /news/articles/cy40pyrnx88o (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000029E3D6E4F50>, 'Connection to www.bbc.com timed out. (connect timeout=30)')) on URL http://www.bbc.com/news/articles/cy40pyrnx88o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com: 100%|██████████| 200/200 [13:40<00:00,  4.10s/it]\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 AI-related articles\n",
      "\n",
      "Attempting to process: http://www.bbc.com/future/article/20250616-how-bibliotherapy-can-both-help-and-harm-your-mental-health\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing news sources:  30%|███       | 3/10 [18:46<56:45, 486.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-related article found!\n",
      "Successfully processed 1 articles\n",
      "\n",
      "Processing source: http://www.cnn.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 11:02:02,768 - CRITICAL - [REQUEST FAILED] HTTPSConnectionPool(host='www.cnn.it', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.cnn.it'. (_ssl.c:1000)\")))\n",
      "2025-08-18 11:02:04,608 - WARNING - Deleting category http://www.cnn.it from source http://www.cnn.com due to download error\n",
      "2025-08-18 11:02:04,608 - WARNING - Deleting category http://www.cnn.it from source http://www.cnn.com due to download error\n",
      "2025-08-18 11:02:05,338 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.cnn.com/feed\n",
      "2025-08-18 11:02:05,338 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.cnn.com/feed\n",
      "2025-08-18 11:02:05,664 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.cnn.com/feeds\n",
      "2025-08-18 11:02:05,664 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.cnn.com/feeds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 594 articles at http://www.cnn.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.cnn.com:  10%|█         | 20/200 [00:09<01:23,  2.14it/s]\n",
      "Scanning articles from http://www.cnn.com:  10%|█         | 20/200 [00:09<01:23,  2.14it/s]\n",
      "Processing news sources:  30%|███       | 3/10 [19:00<44:20, 380.09s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 316\u001b[39m\n\u001b[32m    313\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Keyword occurrences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[33m'\u001b[39m\u001b[33mkeyword_occurrences\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 298\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    295\u001b[39m analyzer = AINewsAnalyzer(\u001b[33m'\u001b[39m\u001b[33mCriterios_extendidos.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Analyze sources\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_articles_per_source\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2022\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Analyze content\u001b[39;00m\n\u001b[32m    301\u001b[39m analysis_results, articles_by_theme = analyzer.analyze_content()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 182\u001b[39m, in \u001b[36mAINewsAnalyzer.analyze_sources\u001b[39m\u001b[34m(self, max_articles_per_source, start_year)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[43marticle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     article.parse()\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m article.text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(article.text) < \u001b[32m100\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\newspaper\\article.py:170\u001b[39m, in \u001b[36mArticle.download\u001b[39m\u001b[34m(self, input_html, title, recursion_counter)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_html \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         html = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_html_2XX_only\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28mself\u001b[39m.download_state = ArticleDownloadState.FAILED_RESPONSE\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\newspaper\\network.py:62\u001b[39m, in \u001b[36mget_html_2XX_only\u001b[39m\u001b[34m(url, config, response)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_html_from_response(response)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mget_request_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museragent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m html = _get_html_from_response(response)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.http_success_only:\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# fail if HTTP sends a non 2XX response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\requests\\sessions.py:724\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\requests\\sessions.py:265\u001b[39m, in \u001b[36mSessionRedirectMixin.resolve_redirects\u001b[39m\u001b[34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m.cookies, prepared_request, resp.raw)\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\urllib3\\connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    788\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\urllib3\\connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    476\u001b[39m         context.load_cert_chain(certfile, keyfile, key_password)\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gagig\\Downloads\\citradi\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    521\u001b[39m     SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1041\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1038\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1039\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1319\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1318\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class AINewsAnalyzer:\n",
    "    def __init__(self, themes_csv_path, news_sources=None):\n",
    "        \"\"\"\n",
    "        Initialize the AI News Analyzer with themes from CSV\n",
    "\n",
    "        Args:\n",
    "            themes_csv_path (str): Path to CSV file containing themes and keywords\n",
    "            news_sources (list): Optional list of news sources to analyze\n",
    "        \"\"\"\n",
    "        self.themes = ThemeProcessor.process_keywords_from_csv(themes_csv_path)\n",
    "        self.news_sources = news_sources or [\n",
    "            \"https://efe.com/en/\",\n",
    "            \"http://www.wired.com\",\n",
    "            \"http://www.bbc.com\",\n",
    "            \"http://www.cnn.com\",\n",
    "            \"http://www.reuters.com\",\n",
    "            \"http://www.theguardian.com\",\n",
    "            \"http://www.nytimes.com\",\n",
    "            \"https://www.afp.com\",\n",
    "            \"https://www.wired.com\",\n",
    "            \"https://www.theguardian.com/technology\",\n",
    "        ]\n",
    "\n",
    "        self.ai_related_terms = globalKeywords\n",
    "\n",
    "        self.articles_data = []\n",
    "        self.debug_stats = {\n",
    "            \"total_urls_found\": 0,\n",
    "            \"download_failures\": 0,\n",
    "            \"parsing_failures\": 0,\n",
    "            \"ai_related_found\": 0,\n",
    "            \"theme_matched\": 0,\n",
    "        }\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        log_dir = Path(\"logs\")\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / \"ai_news_analyzer.log\"),\n",
    "                logging.StreamHandler(),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def is_ai_related(self, text, title):\n",
    "        \"\"\"Check if article is AI-related and matches themes\"\"\"\n",
    "        combined_text = (text + \" \" + title).lower()\n",
    "\n",
    "        # First check if it's AI-related\n",
    "        found_keywords = [\n",
    "            term\n",
    "            for term in self.ai_related_terms\n",
    "            if re.search(r\"\\b\" + re.escape(term.lower()) + r\"\\b\", combined_text)\n",
    "        ]\n",
    "\n",
    "        if not found_keywords:\n",
    "            return False\n",
    "\n",
    "        # Then check if it matches any of our theme keywords\n",
    "        for theme, keywords in self.themes.items():\n",
    "            if any(\n",
    "                re.search(r\"\\b\" + re.escape(keyword.lower()) + r\"\\b\", combined_text)\n",
    "                for keyword in keywords\n",
    "            ):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def download_and_parse_article(self, article_url):\n",
    "        \"\"\"Download and parse a single article with better error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nAttempting to process: {article_url}\")  # Debug print\n",
    "\n",
    "            article = Article(article_url)\n",
    "            try:\n",
    "                article.download()\n",
    "                time.sleep(1)  # Increased delay to be more polite\n",
    "            except Exception as e:\n",
    "                self.debug_stats[\"download_failures\"] += 1\n",
    "                print(f\"Download failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                article.parse()\n",
    "                article.nlp()\n",
    "            except Exception as e:\n",
    "                self.debug_stats[\"parsing_failures\"] += 1\n",
    "                print(f\"Parsing failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "            # Check if we got actual content\n",
    "            if not article.text or len(article.text) < 100:\n",
    "                print(\"Article too short or empty\")\n",
    "                return None\n",
    "\n",
    "            if self.is_ai_related(article.text, article.title):\n",
    "                self.debug_stats[\"ai_related_found\"] += 1\n",
    "                print(\"AI-related article found!\")\n",
    "\n",
    "                # Match themes and keywords\n",
    "                matched_themes = {}\n",
    "                text = (article.text + \" \" + article.title).lower()\n",
    "\n",
    "                for theme, keywords in self.themes.items():\n",
    "                    matched_keywords = []\n",
    "                    for keyword in keywords:\n",
    "                        if keyword.lower() in text:\n",
    "                            matched_keywords.append(keyword)\n",
    "                    if matched_keywords:\n",
    "                        matched_themes[theme] = matched_keywords\n",
    "\n",
    "                if matched_themes:\n",
    "                    self.debug_stats[\"theme_matched\"] += 1\n",
    "                    return {\n",
    "                        \"url\": article_url,\n",
    "                        \"title\": article.title,\n",
    "                        \"text\": article.text,\n",
    "                        \"summary\": article.summary,\n",
    "                        \"keywords\": article.keywords,\n",
    "                        \"publish_date\": article.publish_date.strftime(\"%Y-%m-%d\")\n",
    "                        if article.publish_date\n",
    "                        else None,\n",
    "                        \"authors\": article.authors,\n",
    "                        \"matched_themes\": matched_themes,\n",
    "                        \"source\": re.findall(\n",
    "                            r\"https?://(?:www\\.)?([^/]+)\", article_url\n",
    "                        )[0],\n",
    "                    }\n",
    "            else:\n",
    "                print(\"Not AI-related\")\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {article_url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sources(self, max_articles_per_source=50, start_year=2014):\n",
    "        \"\"\"Analyze news sources with better debugging\"\"\"\n",
    "        for source_url in tqdm(self.news_sources, desc=\"Processing news sources\"):\n",
    "            try:\n",
    "                print(f\"\\nProcessing source: {source_url}\")\n",
    "\n",
    "                config = Config()\n",
    "                config.request_timeout = 30  # Increased timeout\n",
    "                config.memoize_articles = False\n",
    "                config.fetch_images = False\n",
    "\n",
    "                # Build source object\n",
    "                source = newspaper.build(\n",
    "                    source_url, config=config, language=\"en\", number_threads=1\n",
    "                )\n",
    "\n",
    "                print(f\"Found {len(source.articles)} articles at {source_url}\")\n",
    "                self.debug_stats[\"total_urls_found\"] += len(source.articles)\n",
    "\n",
    "                # Get article URLs\n",
    "                ai_related_urls = []\n",
    "                normalized_urls_set = set()\n",
    "                for article in tqdm(\n",
    "                    source.articles[: max_articles_per_source * 2],\n",
    "                    desc=f\"Scanning articles from {source_url}\",\n",
    "                ):\n",
    "                    if not article.url:\n",
    "                        continue\n",
    "\n",
    "                    # Normalize url\n",
    "                    normalized_url = re.sub(r\"^https?://\", \"\", article.url)\n",
    "\n",
    "                    # Avoid duplicates\n",
    "                    if normalized_url in normalized_urls_set:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "\n",
    "                        if not article.text or len(article.text) < 100:\n",
    "                            continue\n",
    "\n",
    "                        if article.publish_date:\n",
    "                            article_year = article.publish_date.year\n",
    "                            if start_year <= article_year <= datetime.now().year:\n",
    "                                if self.is_ai_related(article.text, article.title):\n",
    "                                    ai_related_urls.append(article.url)\n",
    "                                    normalized_urls_set.add(normalized_url)\n",
    "                                    print(f\"Found AI article: {article.url}\")\n",
    "\n",
    "                        if len(ai_related_urls) >= max_articles_per_source:\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing article: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"Found {len(ai_related_urls)} AI-related articles\")\n",
    "\n",
    "                # Process the found articles\n",
    "                with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "                    results = list(\n",
    "                        executor.map(self.download_and_parse_article, ai_related_urls)\n",
    "                    )\n",
    "\n",
    "                valid_results = [r for r in results if r is not None]\n",
    "                self.articles_data.extend(valid_results)\n",
    "\n",
    "                print(f\"Successfully processed {len(valid_results)} articles\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing source {source_url}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def analyze_content(self):\n",
    "        \"\"\"Analyze collected articles for themes\"\"\"\n",
    "        analysis_results = defaultdict(lambda: defaultdict(int))\n",
    "        articles_by_theme = defaultdict(list)\n",
    "\n",
    "        for article in tqdm(self.articles_data, desc=\"Analyzing articles\"):\n",
    "            for theme, keywords in article[\"matched_themes\"].items():\n",
    "                analysis_results[theme][\"articles_count\"] += 1\n",
    "                analysis_results[theme][\"keyword_occurrences\"] += len(keywords)\n",
    "\n",
    "                articles_by_theme[theme].append(\n",
    "                    {\n",
    "                        \"url\": article[\"url\"],\n",
    "                        \"title\": article[\"title\"],\n",
    "                        \"publish_date\": article[\"publish_date\"],\n",
    "                        \"keywords_found\": keywords,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return analysis_results, articles_by_theme\n",
    "\n",
    "    def save_results(\n",
    "        self, analysis_results, articles_by_theme, output_prefix=\"ai_news_analysis\"\n",
    "    ):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        from collections import defaultdict\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        output_dir = Path(\"results\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Deduplicate articles in each theme (by title)\n",
    "        deduped_articles_by_theme = defaultdict(list)\n",
    "        for theme, articles in articles_by_theme.items():\n",
    "            seen_titles = set()\n",
    "            for article in articles:\n",
    "                if article[\"title\"] not in seen_titles:\n",
    "                    deduped_articles_by_theme[theme].append(article)\n",
    "                    seen_titles.add(article[\"title\"])\n",
    "\n",
    "        # Create complete report with deduped articles\n",
    "        report = {\n",
    "            \"summary\": {\n",
    "                \"total_articles\": len(self.articles_data),\n",
    "                \"analysis_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"themes_analyzed\": list(self.themes.keys()),\n",
    "            },\n",
    "            \"theme_analysis\": {\n",
    "                theme: {\n",
    "                    # Use deduplicated article count\n",
    "                    \"articles_count\": len(deduped_articles_by_theme[theme]),\n",
    "                    \"keyword_occurrences\": data[\"keyword_occurrences\"],\n",
    "                    \"articles\": deduped_articles_by_theme[theme],\n",
    "                }\n",
    "                for theme, data in analysis_results.items()\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Save JSON report\n",
    "        with open(\n",
    "            output_dir / f\"{output_prefix}_report.json\", \"w\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Create theme analysis DataFrame (deduped counts)\n",
    "        theme_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"theme\": theme,\n",
    "                    \"articles_count\": len(deduped_articles_by_theme[theme]),\n",
    "                    \"keyword_occurrences\": data[\"keyword_occurrences\"],\n",
    "                    \"percentage_of_total\": (\n",
    "                        len(deduped_articles_by_theme[theme])\n",
    "                        / len(self.articles_data)\n",
    "                        * 100\n",
    "                    )\n",
    "                    if self.articles_data\n",
    "                    else 0,\n",
    "                }\n",
    "                for theme, data in analysis_results.items()\n",
    "            ]\n",
    "        )\n",
    "        theme_df.to_csv(output_dir / f\"{output_prefix}_theme_analysis.csv\", index=False)\n",
    "\n",
    "        # Create articles DataFrame (deduped)\n",
    "        articles_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"theme\": theme,\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"url\": article[\"url\"],\n",
    "                    \"publish_date\": article[\"publish_date\"],\n",
    "                    \"keywords\": \", \".join(article[\"keywords_found\"]),\n",
    "                }\n",
    "                for theme, articles in deduped_articles_by_theme.items()\n",
    "                for article in articles\n",
    "            ]\n",
    "        )\n",
    "        articles_df.to_csv(output_dir / f\"{output_prefix}_articles.csv\", index=False)\n",
    "\n",
    "        logging.info(\n",
    "            f\"Results saved in 'results' directory with prefix '{output_prefix}'\"\n",
    "        )\n",
    "        return report\n",
    "\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize analyzer with your CSV file\n",
    "    analyzer = AINewsAnalyzer(\"Criterios.csv\")\n",
    "\n",
    "    # Analyze sources\n",
    "    analyzer.analyze_sources(max_articles_per_source=100, start_year=2022)\n",
    "\n",
    "    # Analyze content\n",
    "    analysis_results, articles_by_theme = analyzer.analyze_content()\n",
    "\n",
    "    # Save and get report\n",
    "    report = analyzer.save_results(analysis_results, articles_by_theme)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"Total articles analyzed: {report['summary']['total_articles']}\")\n",
    "    print(\"\\nResults by theme:\")\n",
    "    for theme, data in report[\"theme_analysis\"].items():\n",
    "        print(f\"\\n{theme}:\")\n",
    "        print(f\"  Articles: {data['articles_count']}\")\n",
    "        print(f\"  Keyword occurrences: {data['keyword_occurrences']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForumAnalyzer:\n",
    "    def __init__(self, themes_csv_path, reddit_credentials=None, github_token=None):\n",
    "        \"\"\"\n",
    "        Initialize the Forum Analyzer\n",
    "\n",
    "        Args:\n",
    "            themes_csv_path (str): Path to CSV file containing themes and keywords\n",
    "            reddit_credentials (dict): Dictionary with Reddit API credentials\n",
    "            github_token (str): GitHub personal access token\n",
    "        \"\"\"\n",
    "        self.themes = ThemeProcessor.process_keywords_from_csv(themes_csv_path)\n",
    "        self.articles_data = []\n",
    "\n",
    "        # Initialize Reddit client if credentials provided\n",
    "        self.reddit = None\n",
    "        if reddit_credentials:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=reddit_credentials['client_id'],\n",
    "                client_secret=reddit_credentials['client_secret'],\n",
    "                user_agent=reddit_credentials['user_agent']\n",
    "            )\n",
    "\n",
    "        # Initialize GitHub client if token provided\n",
    "        self.github = None\n",
    "        if github_token:\n",
    "            self.github = Github(github_token)\n",
    "\n",
    "        # Initialize logging\n",
    "        self.setup_logging()\n",
    "\n",
    "        # AI-related terms (inherited from AINewsAnalyzer)\n",
    "        self.ai_related_terms = globalKeywords\n",
    "\n",
    "        # Debug stats\n",
    "        self.debug_stats = defaultdict(int)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        log_dir = Path('logs')\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / 'forum_analyzer.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def is_ai_related(self, text, title=\"\"):\n",
    "        \"\"\"Check if content is AI-related and matches themes\"\"\"\n",
    "        combined_text = (text + \" \" + title).lower()\n",
    "\n",
    "        # First check if it's AI-related\n",
    "        if not any(term.lower() in combined_text for term in self.ai_related_terms):\n",
    "            return False\n",
    "\n",
    "        # Then check if it matches any of our theme keywords\n",
    "        for theme, keywords in self.themes.items():\n",
    "            if any(keyword.lower() in combined_text for keyword in keywords):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def analyze_reddit(self, subreddits=['artificial', 'MachineLearning'],\n",
    "                      time_filter='year', limit=1000):\n",
    "        \"\"\"\n",
    "        Analyze Reddit posts from specified subreddits\n",
    "\n",
    "        Args:\n",
    "            subreddits (list): List of subreddit names to analyze\n",
    "            time_filter (str): One of 'day', 'week', 'month', 'year', 'all'\n",
    "            limit (int): Maximum number of posts to analyze per subreddit\n",
    "        \"\"\"\n",
    "        if not self.reddit:\n",
    "            logging.error(\"Reddit client not initialized. Please provide credentials.\")\n",
    "            return\n",
    "\n",
    "        for subreddit_name in tqdm(subreddits, desc=\"Processing subreddits\"):\n",
    "            try:\n",
    "                subreddit = self.reddit.subreddit(subreddit_name)\n",
    "\n",
    "                # Get top posts\n",
    "                for post in tqdm(subreddit.top(time_filter=time_filter, limit=limit),\n",
    "                               desc=f\"Analyzing posts from r/{subreddit_name}\"):\n",
    "\n",
    "                    # Combine post title, content and top comments\n",
    "                    post_text = f\"{post.title} {post.selftext}\"\n",
    "\n",
    "                    # Add top comments\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    comments_text = \" \".join([comment.body for comment in post.comments.list()[:10]])\n",
    "\n",
    "                    combined_text = post_text + \" \" + comments_text\n",
    "\n",
    "                    if self.is_ai_related(combined_text, post.title):\n",
    "                        self.debug_stats['reddit_ai_related'] += 1\n",
    "\n",
    "                        # Match themes and keywords\n",
    "                        matched_themes = {}\n",
    "                        text = combined_text.lower()\n",
    "\n",
    "                        for theme, keywords in self.themes.items():\n",
    "                            matched_keywords = [k for k in keywords if k.lower() in text]\n",
    "                            if matched_keywords:\n",
    "                                matched_themes[theme] = matched_keywords\n",
    "\n",
    "                        if matched_themes:\n",
    "                            self.articles_data.append({\n",
    "                                'url': f\"https://reddit.com{post.permalink}\",\n",
    "                                'title': post.title,\n",
    "                                'text': combined_text,\n",
    "                                'summary': post.selftext[:500] if post.selftext else \"\",\n",
    "                                'publish_date': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d'),\n",
    "                                'author': str(post.author),\n",
    "                                'matched_themes': matched_themes,\n",
    "                                'source': f\"reddit/r/{subreddit_name}\",\n",
    "                                'score': post.score,\n",
    "                                'num_comments': post.num_comments\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing subreddit {subreddit_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def analyze_github(self, query='artificial intelligence', sort='stars',\n",
    "                      max_repos=100, min_stars=100):\n",
    "        \"\"\"\n",
    "        Analyze GitHub repositories (description and readme only, omite discussions for compatibility)\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query for repositories\n",
    "            sort (str): How to sort results ('stars', 'forks', 'updated')\n",
    "            max_repos (int): Maximum number of repositories to analyze\n",
    "            min_stars (int): Minimum number of stars for a repository\n",
    "        \"\"\"\n",
    "        if not self.github:\n",
    "            logging.error(\"GitHub client not initialized. Please provide token.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Search repositories\n",
    "            repositories = self.github.search_repositories(\n",
    "                query=f\"{query} stars:>={min_stars}\",\n",
    "                sort=sort,\n",
    "                order='desc'\n",
    "            )\n",
    "\n",
    "            for repo in tqdm(repositories[:max_repos], desc=\"Analyzing GitHub repositories\"):\n",
    "                try:\n",
    "                    # Combine repository description and readme (omit discussions for compatibility)\n",
    "                    repo_text = f\"{repo.description or ''}\"\n",
    "\n",
    "                    try:\n",
    "                        readme = repo.get_readme().decoded_content.decode()\n",
    "                        repo_text += \" \" + readme\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Get discussions if available (doesn't work)\n",
    "                    # if repo.has_discussions:\n",
    "                    #     discussions = repo.get_discussions()\n",
    "                    #     for discussion in discussions[:10]:  # Get first 10 discussions\n",
    "                    #         repo_text += f\" {discussion.title} {discussion.body}\"\n",
    "\n",
    "                    if self.is_ai_related(repo_text, repo.name):\n",
    "                        self.debug_stats['github_ai_related'] += 1\n",
    "\n",
    "                        # Match themes and keywords\n",
    "                        matched_themes = {}\n",
    "                        text = repo_text.lower()\n",
    "\n",
    "                        for theme, keywords in self.themes.items():\n",
    "                            matched_keywords = [k for k in keywords if k.lower() in text]\n",
    "                            if matched_keywords:\n",
    "                                matched_themes[theme] = matched_keywords\n",
    "\n",
    "                        if matched_themes:\n",
    "                            self.articles_data.append({\n",
    "                                'url': repo.html_url,\n",
    "                                'title': repo.name,\n",
    "                                'text': repo_text[:5000],  # Limit text length\n",
    "                                'summary': repo.description or \"\",\n",
    "                                'publish_date': repo.created_at.strftime('%Y-%m-%d'),\n",
    "                                'author': repo.owner.login,\n",
    "                                'matched_themes': matched_themes,\n",
    "                                'source': 'github',\n",
    "                                'stars': repo.stargazers_count,\n",
    "                                'forks': repo.forks_count\n",
    "                            })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing repository {repo.full_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error searching GitHub repositories: {str(e)}\")\n",
    "\n",
    "    def save_results(self, output_prefix='forum_analysis'):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        output_dir = Path('results')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Create complete report\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_posts': len(self.articles_data),\n",
    "                'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'themes_analyzed': list(self.themes.keys()),\n",
    "                'debug_stats': dict(self.debug_stats)\n",
    "            },\n",
    "            'theme_analysis': defaultdict(lambda: {'posts': [], 'count': 0})\n",
    "        }\n",
    "\n",
    "        # Organize posts by theme\n",
    "        for post in self.articles_data:\n",
    "            for theme in post['matched_themes'].keys():\n",
    "                report['theme_analysis'][theme]['posts'].append({\n",
    "                    'url': post['url'],\n",
    "                    'title': post['title'],\n",
    "                    'source': post['source'],\n",
    "                    'publish_date': post['publish_date']\n",
    "                })\n",
    "                report['theme_analysis'][theme]['count'] += 1\n",
    "\n",
    "        # Convert defaultdict to regular dict for JSON serialization\n",
    "        report['theme_analysis'] = dict(report['theme_analysis'])\n",
    "\n",
    "        # Save files\n",
    "        with open(output_dir / f'{output_prefix}_report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Create DataFrame for analysis\n",
    "        posts_df = pd.DataFrame(self.articles_data)\n",
    "        posts_df.to_csv(output_dir / f'{output_prefix}_posts.csv', index=False)\n",
    "\n",
    "        logging.info(f\"Results saved in 'results' directory with prefix '{output_prefix}'\")\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Reddit API credentials (you'll need to get these from Reddit)\n",
    "    reddit_credentials = {\n",
    "        'client_id': 'vopzVv6U6FpL5xZXQlcifA',\n",
    "        'client_secret': 'rrY4l3SyjVWWFF8Tfx4hJSGTFyVb8A',\n",
    "        'user_agent': 'python:ai_forum_analyzer:v1.0:Investigacion'\n",
    "    }\n",
    "\n",
    "    # GitHub personal access token (you'll need to create this)\n",
    "    github_token = ''\n",
    "\n",
    "    # Initialize analyzer\n",
    "    analyzer = ForumAnalyzer(\n",
    "        'Criterios_extendidos.csv',\n",
    "        reddit_credentials=reddit_credentials,\n",
    "        github_token=github_token\n",
    "    )\n",
    "\n",
    "    # Analyze different platforms\n",
    "    analyzer.analyze_reddit()\n",
    "    analyzer.analyze_github()\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    report = analyzer.save_results()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"Total posts analyzed: {report['summary']['total_posts']}\")\n",
    "    print(\"\\nResults by theme:\")\n",
    "    for theme, data in report['theme_analysis'].items():\n",
    "        print(f\"\\n{theme}:\")\n",
    "        print(f\"  Posts: {data['count']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
