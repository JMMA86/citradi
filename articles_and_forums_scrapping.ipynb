{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: lxml[html_clean] in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.3.0)\n",
      "Requirement already satisfied: lxml-html-clean in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lxml[html_clean]) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: newspaper3k in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (4.12.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (1.3.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (5.3.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (2.32.3)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (5.3.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\gagig\\appdata\\roaming\\python\\python312\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
      "Requirement already satisfied: six in c:\\users\\gagig\\appdata\\roaming\\python\\python312\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gagig\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: praw in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: PyGithub in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.14.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (2.32.3)\n",
      "Requirement already satisfied: pyjwt>=2.4.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyGithub) (2.2.3)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.4.0->PyGithub) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.4.0->PyGithub) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.14.0->PyGithub) (2024.8.30)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gagig\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gagig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gagig\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml[html_clean]\n",
    "%pip install newspaper3k\n",
    "%pip install praw\n",
    "%pip install PyGithub\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from github import Github\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "import newspaper\n",
    "from newspaper import Article, Config\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import praw\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gagig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThemeProcessor:\n",
    "    @staticmethod\n",
    "    def process_keywords_from_csv(csv_path):\n",
    "        \"\"\"Process keywords from CSV file\"\"\"\n",
    "        df = pd.read_csv(csv_path, sep=';', header=None)  # Asumimos que no hay headers, si los hay, ajustar\n",
    "        themes_dict = defaultdict(list)\n",
    "\n",
    "        # Por cada columna en el DataFrame\n",
    "        for col in df.columns:\n",
    "            current_theme = None\n",
    "            for val in df[col].dropna():\n",
    "                line = str(val).strip()\n",
    "\n",
    "                # Si la línea inicia con '===' y finaliza con '===', es un nuevo tema\n",
    "                if line.startswith('===') and line.endswith('==='):\n",
    "                    # Extraemos el nombre del tema quitando los '==='\n",
    "                    # Por ejemplo: === Inclusive growth, sustainable development and well-being ===\n",
    "                    # Queremos quedarnos solo con el texto interno\n",
    "                    theme_name = line.strip('=').strip()\n",
    "                    current_theme = theme_name\n",
    "                    if current_theme not in themes_dict:\n",
    "                        themes_dict[current_theme] = []\n",
    "                \n",
    "                # Si la línea empieza con '-', es un subtema asociado al tema actual\n",
    "                elif line.startswith('-') and current_theme:\n",
    "                    subtopic = line.lstrip('-').strip()\n",
    "                    if subtopic:\n",
    "                        themes_dict[current_theme].append(subtopic)\n",
    "\n",
    "        # Remover duplicados en las listas (opcional)\n",
    "        for theme in themes_dict:\n",
    "            themes_dict[theme] = list(set(themes_dict[theme]))\n",
    "\n",
    "        return dict(themes_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIGen Keywords\n",
    "# globalKeywords = ['AI-generated content','generative AI','deepfakes','attention model','BERT/GPT-style model','LLM','LLMs','large language model','large language models','neural language model','neural language models','language model','language models','machine-generated content','synthetic media','creative AI','generation models','fake media','manipulated media','virtual agents','conversational models']\n",
    "# DeepSeek Keywords\n",
    "# globalKeywords = ['DeepSeek','Deep Seek','DeepSeek's']\n",
    "# OpenAI Keywords\n",
    "# globalKeywords = [\"OpenAI\",\"Open AI\",\"OpenAI's\",\"Open AI's\",\"ChatGPT\",\"chatgpt\",\"Chat GPT\",\"GPT\",\"GPT model\",\"GPT-models\",\"gpt models\",\"GPT-3\",\"gpt 3.5\",\"GPT-4\",\"gpt-4o\",\"OpenAI Codex\",\"OpenAi DALL·E\",\"OpenAI Whisper\",\"GPT-powered assistant\"]\n",
    "# Google Keywords\n",
    "globalKeywords = [\"Google AI\",\"Google Ai\",\"GoogleAI\",\"Google A.I.\",\"Google Artificial Intelligence\",\"Google's AI\",\"Google’s artificial intelligence\",\"Google language model\",\"Google's language model\",\"Google LLM\",\"Google LLMs\",\"Google’s LLMs\",\"Google models\",\"Google’s models\",\"Google AI tools\",\"Google AI models\",\"Google Gemini\",\"Google Bard\",\"Google Deepmind\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing news sources:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing source: https://efe.com/en/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:25:37,167 - CRITICAL - [REQUEST FAILED] 500 Server Error: Internal Server Error for url: https://efe.com/feed/\n",
      "2025-08-07 16:25:38,751 - CRITICAL - [REQUEST FAILED] 500 Server Error: Internal Server Error for url: https://efe.com/feed/\n",
      "2025-08-07 16:25:40,663 - CRITICAL - [REQUEST FAILED] 500 Server Error: Internal Server Error for url: https://efe.com/feed/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 articles at https://efe.com/en/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with Exceeded 30 redirects. on URL https://efe.com/en/latest-news/2025-08-06/frances-largest-fire-in-76-years-is-still-out-of-control/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from https://efe.com/en/: 100%|██████████| 18/18 [00:21<00:00,  1.20s/it]\n",
      "Processing news sources:  10%|█         | 1/10 [00:30<04:36, 30.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: http://www.wired.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:28:30,634 - CRITICAL - [REQUEST FAILED] HTTPSConnectionPool(host='www.wired.cz', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000014E2F5AC770>, 'Connection to www.wired.cz timed out. (connect timeout=30)'))\n",
      "2025-08-07 16:28:33,834 - WARNING - Deleting category https://www.wired.cz from source http://www.wired.com due to download error\n",
      "2025-08-07 16:28:34,241 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.wired.com/feeds\n",
      "2025-08-07 16:28:34,612 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.wired.com/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 317 articles at http://www.wired.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wired.com/v2/offers/wir_edit_hardcoded?source=Site_0_HCL_WIR_EDIT_HARDCODED_HOMEPAGE_MODULE_0_GLOBAL_JULY_2025_NEW_OFFER_ZZ on URL https://www.wired.com/v2/offers/wir_edit_hardcoded?source=Site_0_HCL_WIR_EDIT_HARDCODED_HOMEPAGE_MODULE_0_GLOBAL_JULY_2025_NEW_OFFER_ZZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/big-interview on URL http://www.wired.com/video/series/big-interview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/incognito-mode on URL http://www.wired.com/video/series/incognito-mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/wired-s-50-most-searched-questions on URL http://www.wired.com/video/series/wired-s-50-most-searched-questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.wired.com: 100%|██████████| 200/200 [01:44<00:00,  1.91it/s]\n",
      "Processing news sources:  20%|██        | 2/10 [04:48<21:52, 164.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: http://www.bbc.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:30:46,167 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.bbc.com/undefined\n",
      "2025-08-07 16:31:11,697 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.bbc.com/feed\n",
      "2025-08-07 16:31:15,266 - CRITICAL - [REQUEST FAILED] Exceeded 30 redirects.\n",
      "2025-08-07 16:31:18,032 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.bbc.com/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 549 articles at http://www.bbc.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /news/articles/cx29wjg3vz2o (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000014E2F570EC0>, 'Connection to www.bbc.com timed out. (connect timeout=30)')) on URL http://www.bbc.com/news/articles/cx29wjg3vz2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=30) on URL http://www.bbc.com/reel/video/p0lt0791/pedro-pascal-joins-marvel-after-lemonade-stand-ambush\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /future/article/20250730-the-week-that-covid-hit-britain-returned-to-the-pub (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000014E31E01520>, 'Connection to www.bbc.com timed out. (connect timeout=30)')) on URL http://www.bbc.com/future/article/20250730-the-week-that-covid-hit-britain-returned-to-the-pub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Max retries exceeded with url: /news/articles/cqxg8v74d8jo (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000014E563FCF50>, 'Connection to www.bbc.com timed out. (connect timeout=30)')) on URL http://www.bbc.com/news/articles/cqxg8v74d8jo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=30) on URL http://www.bbc.com/reel/video/p0fsc7kn/how-an-advanced-civilisation-vanished-2-500-years-ago\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.bbc.com: 100%|██████████| 200/200 [09:25<00:00,  2.83s/it]\n",
      "Processing news sources:  30%|███       | 3/10 [15:12<43:39, 374.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: http://www.cnn.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:40:48,986 - CRITICAL - [REQUEST FAILED] HTTPSConnectionPool(host='www.cnn.it', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.cnn.it'. (_ssl.c:1000)\")))\n",
      "2025-08-07 16:40:50,598 - WARNING - Deleting category http://www.cnn.it from source http://www.cnn.com due to download error\n",
      "2025-08-07 16:40:51,245 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.cnn.com/feed\n",
      "2025-08-07 16:40:51,371 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.cnn.com/feeds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 705 articles at http://www.cnn.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.cnn.com: 100%|██████████| 200/200 [03:34<00:00,  1.07s/it]\n",
      "Processing news sources:  40%|████      | 4/10 [18:54<31:26, 314.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: http://www.reuters.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:44:26,677 - WARNING - fromstring() returned an invalid string: ...\n",
      "2025-08-07 16:44:26,689 - WARNING - Source http://www.reuters.com parse error.\n",
      "2025-08-07 16:44:26,692 - CRITICAL - Must extract urls from either html, text or doc!\n",
      "2025-08-07 16:44:26,882 - CRITICAL - [REQUEST FAILED] 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/\n",
      "2025-08-07 16:44:27,065 - CRITICAL - [REQUEST FAILED] 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/feed\n",
      "2025-08-07 16:44:27,261 - CRITICAL - [REQUEST FAILED] 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/feeds\n",
      "2025-08-07 16:44:27,490 - CRITICAL - [REQUEST FAILED] 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 articles at http://www.reuters.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.reuters.com: 0it [00:00, ?it/s]\n",
      "Processing news sources:  50%|█████     | 5/10 [18:57<16:49, 201.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: http://www.theguardian.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:44:31,047 - CRITICAL - [REQUEST FAILED] 403 Client Error: Forbidden for url: https://theguardian.newspapers.com/\n",
      "2025-08-07 16:45:11,280 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.theguardian.com/feed\n",
      "2025-08-07 16:45:11,962 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.theguardian.com/feeds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1335 articles at http://www.theguardian.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.theguardian.com: 100%|██████████| 200/200 [03:05<00:00,  1.08it/s]\n",
      "Processing news sources:  60%|██████    | 6/10 [22:48<14:06, 211.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: http://www.nytimes.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:48:21,161 - CRITICAL - [REQUEST FAILED] 403 Client Error: Forbidden for url: https://timesmachine.nytimes.com/browser\n",
      "2025-08-07 16:48:28,450 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.nytimes.com/feed\n",
      "2025-08-07 16:48:28,943 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.nytimes.com/feeds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 231 articles at http://www.nytimes.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from http://www.nytimes.com: 100%|██████████| 200/200 [03:32<00:00,  1.06s/it]\n",
      "Processing news sources:  70%|███████   | 7/10 [26:31<10:46, 215.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: https://www.afp.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:52:08,209 - CRITICAL - [REQUEST FAILED] 403 Client Error: Forbidden for url: https://news.afp.com/\n",
      "2025-08-07 16:52:09,299 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.afp.com/feed\n",
      "2025-08-07 16:52:09,596 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.afp.com/feeds\n",
      "2025-08-07 16:52:09,890 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.afp.com/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 articles at https://www.afp.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.afp.com/fr/charte-sur-la-protection-des-donnees-personnelles on URL https://www.afp.com/fr/charte-sur-la-protection-des-donnees-personnelles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from https://www.afp.com: 100%|██████████| 25/25 [00:14<00:00,  1.67it/s]\n",
      "Processing news sources:  80%|████████  | 8/10 [26:54<05:08, 154.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: https://www.wired.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:54:53,810 - CRITICAL - [REQUEST FAILED] HTTPSConnectionPool(host='www.wired.cz', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000014E567FEB40>, 'Connection to www.wired.cz timed out. (connect timeout=30)'))\n",
      "2025-08-07 16:54:55,623 - WARNING - Deleting category https://www.wired.cz from source https://www.wired.com due to download error\n",
      "2025-08-07 16:54:55,954 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.wired.com/feeds\n",
      "2025-08-07 16:54:56,288 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.wired.com/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 317 articles at https://www.wired.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wired.com/v2/offers/wir_edit_hardcoded?source=Site_0_HCL_WIR_EDIT_HARDCODED_HOMEPAGE_MODULE_0_GLOBAL_JULY_2025_NEW_OFFER_ZZ on URL https://www.wired.com/v2/offers/wir_edit_hardcoded?source=Site_0_HCL_WIR_EDIT_HARDCODED_HOMEPAGE_MODULE_0_GLOBAL_JULY_2025_NEW_OFFER_ZZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/big-interview on URL http://www.wired.com/video/series/big-interview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/incognito-mode on URL http://www.wired.com/video/series/incognito-mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.wired.com/video/series/wired-s-50-most-searched-questions on URL http://www.wired.com/video/series/wired-s-50-most-searched-questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from https://www.wired.com: 100%|██████████| 200/200 [01:34<00:00,  2.12it/s]\n",
      "Processing news sources:  90%|█████████ | 9/10 [30:59<03:02, 182.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n",
      "\n",
      "Processing source: https://www.theguardian.com/technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 16:56:33,161 - CRITICAL - [REQUEST FAILED] 403 Client Error: Forbidden for url: https://theguardian.newspapers.com/\n",
      "2025-08-07 16:57:08,679 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.theguardian.com/feed\n",
      "2025-08-07 16:57:09,803 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://www.theguardian.com/feeds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1337 articles at https://www.theguardian.com/technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning articles from https://www.theguardian.com/technology: 100%|██████████| 200/200 [02:19<00:00,  1.43it/s]\n",
      "Processing news sources: 100%|██████████| 10/10 [34:00<00:00, 204.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 AI-related articles\n",
      "Successfully processed 0 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing articles: 0it [00:00, ?it/s]\n",
      "2025-08-07 16:59:32,342 - INFO - Results saved in 'results' directory with prefix 'ai_news_analysis'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Summary:\n",
      "Total articles analyzed: 0\n",
      "\n",
      "Results by theme:\n"
     ]
    }
   ],
   "source": [
    "class AINewsAnalyzer:\n",
    "    def __init__(self, themes_csv_path, news_sources=None):\n",
    "        \"\"\"\n",
    "        Initialize the AI News Analyzer with themes from CSV\n",
    "\n",
    "        Args:\n",
    "            themes_csv_path (str): Path to CSV file containing themes and keywords\n",
    "            news_sources (list): Optional list of news sources to analyze\n",
    "        \"\"\"\n",
    "        self.themes = ThemeProcessor.process_keywords_from_csv(themes_csv_path)\n",
    "        self.news_sources = news_sources or [\n",
    "            'https://efe.com/en/',\n",
    "            'http://www.wired.com',\n",
    "            'http://www.bbc.com',\n",
    "            'http://www.cnn.com',\n",
    "            'http://www.reuters.com',\n",
    "            'http://www.theguardian.com',\n",
    "            'http://www.nytimes.com',\n",
    "            'https://www.afp.com',\n",
    "            'https://www.wired.com',\n",
    "            'https://www.theguardian.com/technology',\n",
    "\n",
    "        ]\n",
    "\n",
    "        self.ai_related_terms = globalKeywords\n",
    "\n",
    "        self.articles_data = []\n",
    "        self.debug_stats = {\n",
    "            'total_urls_found': 0,\n",
    "            'download_failures': 0,\n",
    "            'parsing_failures': 0,\n",
    "            'ai_related_found': 0,\n",
    "            'theme_matched': 0\n",
    "        }\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        log_dir = Path('logs')\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / 'ai_news_analyzer.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def is_ai_related(self, text, title):\n",
    "        \"\"\"Check if article is AI-related and matches themes\"\"\"\n",
    "        combined_text = (text + \" \" + title).lower()\n",
    "\n",
    "        # First check if it's AI-related\n",
    "        if not any(term.lower() in combined_text for term in self.ai_related_terms):\n",
    "            return False\n",
    "\n",
    "        # Then check if it matches any of our theme keywords\n",
    "        for theme, keywords in self.themes.items():\n",
    "            if any(keyword.lower() in combined_text for keyword in keywords):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def download_and_parse_article(self, article_url):\n",
    "        \"\"\"Download and parse a single article with better error handling\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nAttempting to process: {article_url}\")  # Debug print\n",
    "\n",
    "            article = Article(article_url)\n",
    "            try:\n",
    "                article.download()\n",
    "                time.sleep(1)  # Increased delay to be more polite\n",
    "            except Exception as e:\n",
    "                self.debug_stats['download_failures'] += 1\n",
    "                print(f\"Download failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                article.parse()\n",
    "                article.nlp()\n",
    "            except Exception as e:\n",
    "                self.debug_stats['parsing_failures'] += 1\n",
    "                print(f\"Parsing failed: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "            # Check if we got actual content\n",
    "            if not article.text or len(article.text) < 100:\n",
    "                print(\"Article too short or empty\")\n",
    "                return None\n",
    "\n",
    "            if self.is_ai_related(article.text, article.title):\n",
    "                self.debug_stats['ai_related_found'] += 1\n",
    "                print(\"AI-related article found!\")\n",
    "\n",
    "                # Match themes and keywords\n",
    "                matched_themes = {}\n",
    "                text = (article.text + \" \" + article.title).lower()\n",
    "\n",
    "                for theme, keywords in self.themes.items():\n",
    "                    matched_keywords = []\n",
    "                    for keyword in keywords:\n",
    "                        if keyword.lower() in text:\n",
    "                            matched_keywords.append(keyword)\n",
    "                    if matched_keywords:\n",
    "                        matched_themes[theme] = matched_keywords\n",
    "\n",
    "                if matched_themes:\n",
    "                    self.debug_stats['theme_matched'] += 1\n",
    "                    return {\n",
    "                        'url': article_url,\n",
    "                        'title': article.title,\n",
    "                        'text': article.text,\n",
    "                        'summary': article.summary,\n",
    "                        'keywords': article.keywords,\n",
    "                        'publish_date': article.publish_date.strftime('%Y-%m-%d') if article.publish_date else None,\n",
    "                        'authors': article.authors,\n",
    "                        'matched_themes': matched_themes,\n",
    "                        'source': re.findall(r'https?://(?:www\\.)?([^/]+)', article_url)[0]\n",
    "                    }\n",
    "            else:\n",
    "                print(\"Not AI-related\")\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {article_url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sources(self, max_articles_per_source=50, start_year=2014):\n",
    "        \"\"\"Analyze news sources with better debugging\"\"\"\n",
    "        for source_url in tqdm(self.news_sources, desc=\"Processing news sources\"):\n",
    "            try:\n",
    "                print(f\"\\nProcessing source: {source_url}\")\n",
    "\n",
    "                config = Config()\n",
    "                config.request_timeout = 30  # Increased timeout\n",
    "                config.memoize_articles = False\n",
    "                config.fetch_images = False\n",
    "\n",
    "                # Build source object\n",
    "                source = newspaper.build(\n",
    "                    source_url,\n",
    "                    config=config,\n",
    "                    language='en',\n",
    "                    number_threads=1\n",
    "                )\n",
    "\n",
    "                print(f\"Found {len(source.articles)} articles at {source_url}\")\n",
    "                self.debug_stats['total_urls_found'] += len(source.articles)\n",
    "\n",
    "                # Get article URLs\n",
    "                ai_related_urls = []\n",
    "                for article in tqdm(source.articles[:max_articles_per_source * 2],\n",
    "                                  desc=f\"Scanning articles from {source_url}\"):\n",
    "                    if not article.url:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "\n",
    "                        if not article.text or len(article.text) < 100:\n",
    "                            continue\n",
    "\n",
    "                        if article.publish_date:\n",
    "                            article_year = article.publish_date.year\n",
    "                            if start_year <= article_year <= datetime.now().year:\n",
    "                                if self.is_ai_related(article.text, article.title):\n",
    "                                    ai_related_urls.append(article.url)\n",
    "                                    print(f\"Found AI article: {article.url}\")\n",
    "\n",
    "                        if len(ai_related_urls) >= max_articles_per_source:\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing article: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"Found {len(ai_related_urls)} AI-related articles\")\n",
    "\n",
    "                # Process the found articles\n",
    "                with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "                    results = list(executor.map(self.download_and_parse_article, ai_related_urls))\n",
    "\n",
    "                valid_results = [r for r in results if r is not None]\n",
    "                self.articles_data.extend(valid_results)\n",
    "\n",
    "                print(f\"Successfully processed {len(valid_results)} articles\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing source {source_url}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def analyze_content(self):\n",
    "        \"\"\"Analyze collected articles for themes\"\"\"\n",
    "        analysis_results = defaultdict(lambda: defaultdict(int))\n",
    "        articles_by_theme = defaultdict(list)\n",
    "\n",
    "        for article in tqdm(self.articles_data, desc=\"Analyzing articles\"):\n",
    "            for theme, keywords in article['matched_themes'].items():\n",
    "                analysis_results[theme]['articles_count'] += 1\n",
    "                analysis_results[theme]['keyword_occurrences'] += len(keywords)\n",
    "\n",
    "                articles_by_theme[theme].append({\n",
    "                    'url': article['url'],\n",
    "                    'title': article['title'],\n",
    "                    'publish_date': article['publish_date'],\n",
    "                    'keywords_found': keywords\n",
    "                })\n",
    "\n",
    "        return analysis_results, articles_by_theme\n",
    "\n",
    "    def save_results(self, analysis_results, articles_by_theme, output_prefix='ai_news_analysis'):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        # Ensure output directory exists\n",
    "        output_dir = Path('results')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Create complete report\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_articles': len(self.articles_data),\n",
    "                'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'themes_analyzed': list(self.themes.keys())\n",
    "            },\n",
    "            'theme_analysis': {\n",
    "                theme: {\n",
    "                    'articles_count': data['articles_count'],\n",
    "                    'keyword_occurrences': data['keyword_occurrences'],\n",
    "                    'articles': articles_by_theme[theme]\n",
    "                }\n",
    "                for theme, data in analysis_results.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save files\n",
    "        with open(output_dir / f'{output_prefix}_report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Create theme analysis DataFrame\n",
    "        theme_df = pd.DataFrame([\n",
    "            {\n",
    "                'theme': theme,\n",
    "                'articles_count': data['articles_count'],\n",
    "                'keyword_occurrences': data['keyword_occurrences'],\n",
    "                'percentage_of_total': (data['articles_count'] / len(self.articles_data) * 100) if self.articles_data else 0\n",
    "            }\n",
    "            for theme, data in analysis_results.items()\n",
    "        ])\n",
    "        theme_df.to_csv(output_dir / f'{output_prefix}_theme_analysis.csv', index=False)\n",
    "\n",
    "        # Create articles DataFrame\n",
    "        articles_df = pd.DataFrame([\n",
    "            {\n",
    "                'theme': theme,\n",
    "                'title': article['title'],\n",
    "                'url': article['url'],\n",
    "                'publish_date': article['publish_date'],\n",
    "                'keywords': ', '.join(article['keywords_found'])\n",
    "            }\n",
    "            for theme, articles in articles_by_theme.items()\n",
    "            for article in articles\n",
    "        ])\n",
    "        articles_df.to_csv(output_dir / f'{output_prefix}_articles.csv', index=False)\n",
    "\n",
    "        logging.info(f\"Results saved in 'results' directory with prefix '{output_prefix}'\")\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize analyzer with your CSV file\n",
    "    analyzer = AINewsAnalyzer('Criterios.csv')\n",
    "\n",
    "    # Analyze sources\n",
    "    analyzer.analyze_sources(max_articles_per_source=100, start_year=2022)\n",
    "\n",
    "    # Analyze content\n",
    "    analysis_results, articles_by_theme = analyzer.analyze_content()\n",
    "\n",
    "    # Save and get report\n",
    "    report = analyzer.save_results(analysis_results, articles_by_theme)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"Total articles analyzed: {report['summary']['total_articles']}\")\n",
    "    print(\"\\nResults by theme:\")\n",
    "    for theme, data in report['theme_analysis'].items():\n",
    "        print(f\"\\n{theme}:\")\n",
    "        print(f\"  Articles: {data['articles_count']}\")\n",
    "        print(f\"  Keyword occurrences: {data['keyword_occurrences']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing posts from r/artificial: 153it [06:21,  2.49s/it]]\n",
      "2025-08-07 18:19:59,193 - ERROR - Error processing subreddit artificial: received 429 HTTP response\n",
      "Analyzing posts from r/MachineLearning: 0it [00:00, ?it/s]381.28s/it]\n",
      "2025-08-07 18:19:59,312 - ERROR - Error processing subreddit MachineLearning: received 429 HTTP response\n",
      "Processing subreddits: 100%|██████████| 2/2 [06:21<00:00, 190.70s/it]\n",
      "Analyzing GitHub repositories: 100it [01:09,  1.43it/s]\n",
      "2025-08-07 18:21:09,242 - INFO - Results saved in 'results' directory with prefix 'forum_analysis'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Summary:\n",
      "Total posts analyzed: 6\n",
      "\n",
      "Results by theme:\n",
      "\n",
      "Privacy:\n",
      "  Posts: 4\n",
      "\n",
      "Copyright:\n",
      "  Posts: 4\n"
     ]
    }
   ],
   "source": [
    "class ForumAnalyzer:\n",
    "    def __init__(self, themes_csv_path, reddit_credentials=None, github_token=None):\n",
    "        \"\"\"\n",
    "        Initialize the Forum Analyzer\n",
    "\n",
    "        Args:\n",
    "            themes_csv_path (str): Path to CSV file containing themes and keywords\n",
    "            reddit_credentials (dict): Dictionary with Reddit API credentials\n",
    "            github_token (str): GitHub personal access token\n",
    "        \"\"\"\n",
    "        self.themes = ThemeProcessor.process_keywords_from_csv(themes_csv_path)\n",
    "        self.articles_data = []\n",
    "\n",
    "        # Initialize Reddit client if credentials provided\n",
    "        self.reddit = None\n",
    "        if reddit_credentials:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=reddit_credentials['client_id'],\n",
    "                client_secret=reddit_credentials['client_secret'],\n",
    "                user_agent=reddit_credentials['user_agent']\n",
    "            )\n",
    "\n",
    "        # Initialize GitHub client if token provided\n",
    "        self.github = None\n",
    "        if github_token:\n",
    "            self.github = Github(github_token)\n",
    "\n",
    "        # Initialize logging\n",
    "        self.setup_logging()\n",
    "\n",
    "        # AI-related terms (inherited from AINewsAnalyzer)\n",
    "        self.ai_related_terms = globalKeywords\n",
    "\n",
    "        # Debug stats\n",
    "        self.debug_stats = defaultdict(int)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        log_dir = Path('logs')\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / 'forum_analyzer.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def is_ai_related(self, text, title=\"\"):\n",
    "        \"\"\"Check if content is AI-related and matches themes\"\"\"\n",
    "        combined_text = (text + \" \" + title).lower()\n",
    "\n",
    "        # First check if it's AI-related\n",
    "        if not any(term.lower() in combined_text for term in self.ai_related_terms):\n",
    "            return False\n",
    "\n",
    "        # Then check if it matches any of our theme keywords\n",
    "        for theme, keywords in self.themes.items():\n",
    "            if any(keyword.lower() in combined_text for keyword in keywords):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def analyze_reddit(self, subreddits=['artificial', 'MachineLearning'],\n",
    "                      time_filter='year', limit=1000):\n",
    "        \"\"\"\n",
    "        Analyze Reddit posts from specified subreddits\n",
    "\n",
    "        Args:\n",
    "            subreddits (list): List of subreddit names to analyze\n",
    "            time_filter (str): One of 'day', 'week', 'month', 'year', 'all'\n",
    "            limit (int): Maximum number of posts to analyze per subreddit\n",
    "        \"\"\"\n",
    "        if not self.reddit:\n",
    "            logging.error(\"Reddit client not initialized. Please provide credentials.\")\n",
    "            return\n",
    "\n",
    "        for subreddit_name in tqdm(subreddits, desc=\"Processing subreddits\"):\n",
    "            try:\n",
    "                subreddit = self.reddit.subreddit(subreddit_name)\n",
    "\n",
    "                # Get top posts\n",
    "                for post in tqdm(subreddit.top(time_filter=time_filter, limit=limit),\n",
    "                               desc=f\"Analyzing posts from r/{subreddit_name}\"):\n",
    "\n",
    "                    # Combine post title, content and top comments\n",
    "                    post_text = f\"{post.title} {post.selftext}\"\n",
    "\n",
    "                    # Add top comments\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    comments_text = \" \".join([comment.body for comment in post.comments.list()[:10]])\n",
    "\n",
    "                    combined_text = post_text + \" \" + comments_text\n",
    "\n",
    "                    if self.is_ai_related(combined_text, post.title):\n",
    "                        self.debug_stats['reddit_ai_related'] += 1\n",
    "\n",
    "                        # Match themes and keywords\n",
    "                        matched_themes = {}\n",
    "                        text = combined_text.lower()\n",
    "\n",
    "                        for theme, keywords in self.themes.items():\n",
    "                            matched_keywords = [k for k in keywords if k.lower() in text]\n",
    "                            if matched_keywords:\n",
    "                                matched_themes[theme] = matched_keywords\n",
    "\n",
    "                        if matched_themes:\n",
    "                            self.articles_data.append({\n",
    "                                'url': f\"https://reddit.com{post.permalink}\",\n",
    "                                'title': post.title,\n",
    "                                'text': combined_text,\n",
    "                                'summary': post.selftext[:500] if post.selftext else \"\",\n",
    "                                'publish_date': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d'),\n",
    "                                'author': str(post.author),\n",
    "                                'matched_themes': matched_themes,\n",
    "                                'source': f\"reddit/r/{subreddit_name}\",\n",
    "                                'score': post.score,\n",
    "                                'num_comments': post.num_comments\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing subreddit {subreddit_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def analyze_github(self, query='artificial intelligence', sort='stars',\n",
    "                      max_repos=100, min_stars=100):\n",
    "        \"\"\"\n",
    "        Analyze GitHub repositories (description and readme only, omite discussions for compatibility)\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query for repositories\n",
    "            sort (str): How to sort results ('stars', 'forks', 'updated')\n",
    "            max_repos (int): Maximum number of repositories to analyze\n",
    "            min_stars (int): Minimum number of stars for a repository\n",
    "        \"\"\"\n",
    "        if not self.github:\n",
    "            logging.error(\"GitHub client not initialized. Please provide token.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Search repositories\n",
    "            repositories = self.github.search_repositories(\n",
    "                query=f\"{query} stars:>={min_stars}\",\n",
    "                sort=sort,\n",
    "                order='desc'\n",
    "            )\n",
    "\n",
    "            for repo in tqdm(repositories[:max_repos], desc=\"Analyzing GitHub repositories\"):\n",
    "                try:\n",
    "                    # Combine repository description and readme (omit discussions for compatibility)\n",
    "                    repo_text = f\"{repo.description or ''}\"\n",
    "\n",
    "                    try:\n",
    "                        readme = repo.get_readme().decoded_content.decode()\n",
    "                        repo_text += \" \" + readme\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Get discussions if available (doesn't work)\n",
    "                    # if repo.has_discussions:\n",
    "                    #     discussions = repo.get_discussions()\n",
    "                    #     for discussion in discussions[:10]:  # Get first 10 discussions\n",
    "                    #         repo_text += f\" {discussion.title} {discussion.body}\"\n",
    "\n",
    "                    if self.is_ai_related(repo_text, repo.name):\n",
    "                        self.debug_stats['github_ai_related'] += 1\n",
    "\n",
    "                        # Match themes and keywords\n",
    "                        matched_themes = {}\n",
    "                        text = repo_text.lower()\n",
    "\n",
    "                        for theme, keywords in self.themes.items():\n",
    "                            matched_keywords = [k for k in keywords if k.lower() in text]\n",
    "                            if matched_keywords:\n",
    "                                matched_themes[theme] = matched_keywords\n",
    "\n",
    "                        if matched_themes:\n",
    "                            self.articles_data.append({\n",
    "                                'url': repo.html_url,\n",
    "                                'title': repo.name,\n",
    "                                'text': repo_text[:5000],  # Limit text length\n",
    "                                'summary': repo.description or \"\",\n",
    "                                'publish_date': repo.created_at.strftime('%Y-%m-%d'),\n",
    "                                'author': repo.owner.login,\n",
    "                                'matched_themes': matched_themes,\n",
    "                                'source': 'github',\n",
    "                                'stars': repo.stargazers_count,\n",
    "                                'forks': repo.forks_count\n",
    "                            })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing repository {repo.full_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error searching GitHub repositories: {str(e)}\")\n",
    "\n",
    "    def save_results(self, output_prefix='forum_analysis'):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        output_dir = Path('results')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Create complete report\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_posts': len(self.articles_data),\n",
    "                'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'themes_analyzed': list(self.themes.keys()),\n",
    "                'debug_stats': dict(self.debug_stats)\n",
    "            },\n",
    "            'theme_analysis': defaultdict(lambda: {'posts': [], 'count': 0})\n",
    "        }\n",
    "\n",
    "        # Organize posts by theme\n",
    "        for post in self.articles_data:\n",
    "            for theme in post['matched_themes'].keys():\n",
    "                report['theme_analysis'][theme]['posts'].append({\n",
    "                    'url': post['url'],\n",
    "                    'title': post['title'],\n",
    "                    'source': post['source'],\n",
    "                    'publish_date': post['publish_date']\n",
    "                })\n",
    "                report['theme_analysis'][theme]['count'] += 1\n",
    "\n",
    "        # Convert defaultdict to regular dict for JSON serialization\n",
    "        report['theme_analysis'] = dict(report['theme_analysis'])\n",
    "\n",
    "        # Save files\n",
    "        with open(output_dir / f'{output_prefix}_report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Create DataFrame for analysis\n",
    "        posts_df = pd.DataFrame(self.articles_data)\n",
    "        posts_df.to_csv(output_dir / f'{output_prefix}_posts.csv', index=False)\n",
    "\n",
    "        logging.info(f\"Results saved in 'results' directory with prefix '{output_prefix}'\")\n",
    "        return report\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Reddit API credentials (you'll need to get these from Reddit)\n",
    "    reddit_credentials = {\n",
    "        'client_id': 'vopzVv6U6FpL5xZXQlcifA',\n",
    "        'client_secret': 'rrY4l3SyjVWWFF8Tfx4hJSGTFyVb8A',\n",
    "        'user_agent': 'python:ai_forum_analyzer:v1.0:Investigacion'\n",
    "    }\n",
    "\n",
    "    # GitHub personal access token (you'll need to create this)\n",
    "    github_token = ''\n",
    "\n",
    "    # Initialize analyzer\n",
    "    analyzer = ForumAnalyzer(\n",
    "        'Criterios.csv',\n",
    "        reddit_credentials=reddit_credentials,\n",
    "        github_token=github_token\n",
    "    )\n",
    "\n",
    "    # Analyze different platforms\n",
    "    analyzer.analyze_reddit()\n",
    "    analyzer.analyze_github()\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    report = analyzer.save_results()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(f\"Total posts analyzed: {report['summary']['total_posts']}\")\n",
    "    print(\"\\nResults by theme:\")\n",
    "    for theme, data in report['theme_analysis'].items():\n",
    "        print(f\"\\n{theme}:\")\n",
    "        print(f\"  Posts: {data['count']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
